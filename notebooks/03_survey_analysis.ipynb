{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkiEiCRxSzqn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to raw survey responses (local only, not tracked in GitHub)\n",
        "survey_path = \"survey_raw_responses.csv\"\n",
        "\n",
        "df = pd.read_csv(survey_path)\n",
        "print(f\"Loaded {len(df)} survey responses\")\n"
      ],
      "metadata": {
        "id": "nSH80CxUS8Eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = [c.strip() for c in df.columns]\n"
      ],
      "metadata": {
        "id": "3zn2Dr6_S__b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "likert_questions = {\n",
        "    \"Clarity\": {\n",
        "        \"Grad-CAM\": \"Clarity [Grad-CAM]\",\n",
        "        \"Integrated Gradients\": \"Clarity [Integrated Gradients]\",\n",
        "        \"LIME\": \"Clarity [LIME]\",\n",
        "        \"Saliency\": \"Clarity [Saliency]\"\n",
        "    },\n",
        "    \"Relevance\": {\n",
        "        \"Grad-CAM\": \"Relevance [Grad-CAM]\",\n",
        "        \"Integrated Gradients\": \"Relevance [Integrated Gradients]\",\n",
        "        \"LIME\": \"Relevance [LIME]\",\n",
        "        \"Saliency\": \"Relevance [Saliency]\"\n",
        "    },\n",
        "    \"Trust\": {\n",
        "        \"Grad-CAM\": \"Trust [Grad-CAM]\",\n",
        "        \"Integrated Gradients\": \"Trust [Integrated Gradients]\",\n",
        "        \"LIME\": \"Trust [LIME]\",\n",
        "        \"Saliency\": \"Trust [Saliency]\"\n",
        "    },\n",
        "    \"Usefulness\": {\n",
        "        \"Grad-CAM\": \"Usefulness [Grad-CAM]\",\n",
        "        \"Integrated Gradients\": \"Usefulness [Integrated Gradients]\",\n",
        "        \"LIME\": \"Usefulness [LIME]\",\n",
        "        \"Saliency\": \"Usefulness [Saliency]\"\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "UXqedOk7TBrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "likert_summary = []\n",
        "\n",
        "for metric, methods in likert_questions.items():\n",
        "    for method, col in methods.items():\n",
        "        values = df[col].dropna()\n",
        "        likert_summary.append({\n",
        "            \"category\": \"likert\",\n",
        "            \"metric\": metric.lower(),\n",
        "            \"method\": method,\n",
        "            \"mean\": values.mean(),\n",
        "            \"std\": values.std(),\n",
        "            \"n\": len(values)\n",
        "        })\n",
        "\n",
        "likert_df = pd.DataFrame(likert_summary)\n",
        "likert_df\n"
      ],
      "metadata": {
        "id": "3j9yrKfNTD4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_cols = {\n",
        "    \"Grad-CAM\": \"Overall rating [Grad-CAM]\",\n",
        "    \"Integrated Gradients\": \"Overall rating [Integrated Gradients]\",\n",
        "    \"LIME\": \"Overall rating [LIME]\",\n",
        "    \"Saliency\": \"Overall rating [Saliency]\"\n",
        "}\n",
        "\n",
        "overall_summary = []\n",
        "\n",
        "for method, col in overall_cols.items():\n",
        "    values = df[col].dropna()\n",
        "    overall_summary.append({\n",
        "        \"category\": \"overall\",\n",
        "        \"metric\": \"mean_rating\",\n",
        "        \"method\": method,\n",
        "        \"mean\": values.mean(),\n",
        "        \"std\": values.std(),\n",
        "        \"n\": len(values)\n",
        "    })\n",
        "\n",
        "overall_df = pd.DataFrame(overall_summary)\n",
        "overall_df\n"
      ],
      "metadata": {
        "id": "iE6cMvUlTGGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preference_questions = {\n",
        "    \"Best overall explanation\": \"Best overall explanation\",\n",
        "    \"Best for model debugging\": \"Best for model debugging\",\n",
        "    \"Most interpretable for humans\": \"Most interpretable for humans\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "gL2zlmuCTIPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preference_summary = []\n",
        "\n",
        "for metric, col in preference_questions.items():\n",
        "    counts = df[col].value_counts(normalize=True) * 100\n",
        "    for method, pct in counts.items():\n",
        "        preference_summary.append({\n",
        "            \"category\": \"preference\",\n",
        "            \"metric\": metric.replace(\" \", \"_\").lower(),\n",
        "            \"method\": method,\n",
        "            \"percentage\": pct\n",
        "        })\n",
        "\n",
        "preference_df = pd.DataFrame(preference_summary)\n",
        "preference_df\n"
      ],
      "metadata": {
        "id": "fwSw3X6_TJx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranking_cols = {\n",
        "    \"Grad-CAM\": \"Ranking [Grad-CAM]\",\n",
        "    \"Integrated Gradients\": \"Ranking [Integrated Gradients]\",\n",
        "    \"LIME\": \"Ranking [LIME]\",\n",
        "    \"Saliency\": \"Ranking [Saliency]\"\n",
        "}\n",
        "\n",
        "ranking_summary = []\n",
        "\n",
        "for method, col in ranking_cols.items():\n",
        "    values = df[col].dropna()\n",
        "    ranking_summary.append({\n",
        "        \"category\": \"cognitive_load\",\n",
        "        \"metric\": \"average_rank\",\n",
        "        \"method\": method,\n",
        "        \"mean_rank\": values.mean(),\n",
        "        \"n\": len(values)\n",
        "    })\n",
        "\n",
        "ranking_df = pd.DataFrame(ranking_summary)\n",
        "ranking_df\n"
      ],
      "metadata": {
        "id": "535A5ON0TLu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_df = pd.concat(\n",
        "    [likert_df, overall_df, preference_df, ranking_df],\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "summary_df\n"
      ],
      "metadata": {
        "id": "-PXKPXp8TN1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_df.to_csv(\n",
        "    \"survey_results_summary.csv\",\n",
        "    index=False\n",
        ")\n",
        "\n",
        "print(\"Saved aggregated survey summary to survey_results_summary.csv\")\n"
      ],
      "metadata": {
        "id": "vJ7h9yEETPxV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}