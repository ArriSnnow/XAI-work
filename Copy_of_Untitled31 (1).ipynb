{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Environment Setup**\n",
        "\n",
        "_In this section, we pin our dependencies (NumPy, Captum, etc.) and verify that we are running on the correct framework versions.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rKoOZJtr0suc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── Cell 1: INSTALL CUDA-ENABLED PYTORCH ────────────────────────────────────────\n",
        "# (You can remove the heavy uninstall/rm -rf; a fresh Colab VM comes clean.)\n",
        "\n",
        "# 1) Install matching CUDA PyTorch and TorchVision\n",
        "!pip install --quiet \\\n",
        "    --index-url https://download.pytorch.org/whl/cu121 \\\n",
        "    torch==2.2.0+cu121 torchvision==0.17.0+cu121\n",
        "\n",
        "# 2) Install Captum & SciPy\n",
        "!pip install --quiet captum scipy\n"
      ],
      "metadata": {
        "id": "8Q4Kg_4mEY-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ─── End of installs ─────────────────────────────────────────────────────────────\n",
        "\n",
        "import torch, torchvision\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Device & seed\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# 2. Load model + transforms\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "weights    = ResNet50_Weights.DEFAULT\n",
        "model      = resnet50(weights=weights).to(device).eval()\n",
        "preprocess = weights.transforms()\n",
        "\n",
        "# 3. Confirm\n",
        "print(\"Using device:\", device)\n",
        "print(\"  torch version:\", torch.__version__)\n",
        "print(\"  torchvision:\", torchvision.__version__)\n",
        "print(\"  numpy version:\", np.__version__)\n"
      ],
      "metadata": {
        "id": "lo1zfmsnvHsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── Cell 1.1: Install & import LIME ────────────────────────────\n",
        "!pip install --quiet lime\n",
        "\n",
        "from lime import lime_image\n",
        "from skimage.segmentation import slic\n",
        "from torchvision.transforms import ToPILImage\n",
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "from lime import lime_image\n"
      ],
      "metadata": {
        "id": "C-kix5aLCudk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"Using device:\", device)\n",
        "print(\"CUDA available?\", torch.cuda.is_available())\n",
        "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"—\")\n"
      ],
      "metadata": {
        "id": "hISnEugiqDLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell: Load Pascal VOC 2007 Validation Set\n",
        "\n",
        "This cell downloads and prepares the VOC 2007 validation split, loads a pre-trained ResNet50 (with its recommended preprocessing), and builds a simple in-memory list called `dataset`. Each entry in `dataset` is a tuple  (PIL_Image, [list_of_gt_boxes])` containing the raw image and its ground‐truth bounding boxes. After running, you can use `dataset` directly for downstream evaluation.\n",
        "\n"
      ],
      "metadata": {
        "id": "7apIci3q00Z1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXzvbcnxJMhR"
      },
      "outputs": [],
      "source": [
        "# ─── Cell 2: Load Pascal VOC 2007 val set ──────────────────────────────────────────\n",
        "from torchvision.datasets import VOCDetection\n",
        "from pathlib import Path\n",
        "\n",
        "# (model, device, and preprocess were set in Cell 1; no need to reload them here)\n",
        "\n",
        "# Download & prepare VOC 2007 validation set\n",
        "voc_val = VOCDetection(\n",
        "    root=\"./data\",\n",
        "    year=\"2007\",\n",
        "    image_set=\"val\",\n",
        "    download=True,\n",
        "    transform=None       # keep raw PIL.Image so we can do custom BGR/transform later\n",
        ")\n",
        "\n",
        "# Build our (PIL-image, [list of GT boxes]) pairs\n",
        "dataset = []\n",
        "for img_pil, target in voc_val:\n",
        "    boxes = []\n",
        "    objs = target[\"annotation\"].get(\"object\", [])\n",
        "    if isinstance(objs, dict):\n",
        "        objs = [objs]\n",
        "    for obj in objs:\n",
        "        bb = obj[\"bndbox\"]\n",
        "        boxes.append([\n",
        "            int(bb[\"xmin\"]), int(bb[\"ymin\"]),\n",
        "            int(bb[\"xmax\"]), int(bb[\"ymax\"])\n",
        "        ])\n",
        "    dataset.append((img_pil, boxes))\n",
        "\n",
        "print(f\"VOC dataset ready with {len(dataset)} images\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cell: Helper Functions for CAM, Saliency, and Integrated Gradients**\n",
        "\n",
        "Defines three utility functions:\n",
        "\n",
        "1. **`compute_cam(...)`**  \n",
        "   - Registers hooks on the chosen ResNet50 layer, runs a forward/backward pass, and builds a Grad-CAM heatmap plus overlay for a given BGR image.\n",
        "\n",
        "2. **`compute_saliency(...)`**  \n",
        "   - Runs a vanilla gradient backpropagation to compute a saliency map  (absolute gradient max over channels), then normalizes and resizes it to the input image size.\n",
        "\n",
        "3. **`compute_ig(...)`**  \n",
        "   - Uses Captum’s IntegratedGradients to compute per-pixel attributions collapses channels (absolute sum), normalizes, and resizes back to the original image dimensions.\n",
        "\n",
        "All three functions return a single‐channel heatmap (shape H×W) that can be used for further evaluation or visualization.\n"
      ],
      "metadata": {
        "id": "JkkcMPzQ1Jlj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-_ISK7_IfSL"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def compute_cam(img_bgr, model, preprocess, target_layer=None, device=device):\n",
        "    \"\"\"Returns (heatmap, overlay) for a single BGR image.\"\"\"\n",
        "    # 1) Hook storage\n",
        "    activations, gradients = {}, {}\n",
        "\n",
        "    if target_layer is None:\n",
        "        target_layer = model.layer4[-1]\n",
        "\n",
        "    def fwd_hook(mod, inp, out):    activations['feat'] = out.detach()\n",
        "    def bwd_hook(mod, grad_in, grad_out): gradients['grad'] = grad_out[0].detach()\n",
        "\n",
        "    h_f = target_layer.register_forward_hook(fwd_hook)\n",
        "    h_b = target_layer.register_backward_hook(bwd_hook)\n",
        "\n",
        "    # 2) Preprocess\n",
        "    rgb    = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    pil = Image.fromarray(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))\n",
        "    inp = preprocess(pil).unsqueeze(0).to(device)\n",
        "\n",
        "    # 3) Forward/backward\n",
        "    out = model(inp)\n",
        "    cls = out.argmax(dim=1).item()\n",
        "    model.zero_grad()\n",
        "    out[0, cls].backward()\n",
        "\n",
        "    # 4) Build CAM\n",
        "    feat = activations['feat'][0].cpu().numpy()      # C×H×W\n",
        "    grad = gradients['grad'][0].cpu().numpy()       # C×H×W\n",
        "    weights_ = grad.mean(axis=(1,2))                # C\n",
        "    cam = np.zeros(feat.shape[1:], dtype=np.float32)\n",
        "    for i, w in enumerate(weights_):\n",
        "        cam += w * feat[i]\n",
        "    cam = np.maximum(cam, 0)\n",
        "    cam = cv2.resize(cam, (img_bgr.shape[1], img_bgr.shape[0]))\n",
        "    heatmap = (cam - cam.min())/(cam.max()-cam.min()+1e-8)\n",
        "\n",
        "    # 5) Overlay\n",
        "    colored = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
        "    overlay = cv2.addWeighted(colored, 0.5, img_bgr, 0.5, 0)\n",
        "\n",
        "    # 6) Clean up hooks\n",
        "    h_f.remove(); h_b.remove()\n",
        "    return heatmap, overlay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Cell: compare first block vs last block of ResNet ──────────────────────────\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch # Import torch as it's used in compute_cam\n",
        "from torchvision.models import resnet50, ResNet50_Weights # Import these as they are used\n",
        "\n",
        "early_layer = model.layer1[0]\n",
        "late_layer  = model.layer4[-1]\n",
        "\n",
        "# Load an image from the dataset\n",
        "img_pil, _ = dataset[0] # Assuming dataset is a list of (image, boxes) tuples.\n",
        "\n",
        "# Convert PIL image to OpenCV BGR image\n",
        "img_bgr = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "heat_early, overlay_early = compute_cam(img_bgr, model, preprocess,\n",
        "                                       target_layer=early_layer,\n",
        "                                       device=device)\n",
        "heat_late,  overlay_late  = compute_cam(img_bgr, model, preprocess,\n",
        "                                       target_layer=late_layer,\n",
        "                                       device=device)"
      ],
      "metadata": {
        "id": "NyMuD6mxLcLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7LxTMWsIhZ2"
      },
      "outputs": [],
      "source": [
        "def compute_saliency(img_bgr, model, preprocess, device=device):\n",
        "    \"\"\"Vanilla gradient saliency, resized to the original image size.\"\"\"\n",
        "    model.zero_grad()\n",
        "    # 1) Preprocess to model input size\n",
        "    rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    pil = Image.fromarray(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))\n",
        "    inp = preprocess(pil).unsqueeze(0).to(device)\n",
        "    inp.requires_grad_(True)\n",
        "\n",
        "    # 2) Forward\n",
        "    model.zero_grad()\n",
        "    out = model(inp)\n",
        "    cls = out.argmax(dim=1).item()\n",
        "\n",
        "    # 3) Backward on the *scalar* score for that class\n",
        "    score = out[0, cls]\n",
        "    score.backward()\n",
        "\n",
        "    # 4) Extract gradient w.r.t. input, sum channels → H×W\n",
        "    grad = inp.grad.abs()[0].cpu().numpy()     # shape: 3×h×w\n",
        "    sal = grad.max(axis=0)                     # shape: h×w\n",
        "\n",
        "    # 4) Normalize\n",
        "    sal = (sal - sal.min())/(sal.max() - sal.min() + 1e-8)\n",
        "\n",
        "    # 5) Normalize & resize back to original\n",
        "    sal = (sal - sal.min()) / (sal.max() - sal.min() + 1e-8)\n",
        "    H, W = img_bgr.shape[:2]\n",
        "    sal = cv2.resize(sal, (W, H))\n",
        "\n",
        "    return sal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gI3QPOeiSPMY"
      },
      "outputs": [],
      "source": [
        "# Cell 8: redefine compute_ig to accept a target_layer\n",
        "from captum.attr import IntegratedGradients, LayerIntegratedGradients\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "def compute_ig(\n",
        "    img_bgr,           # H×W×3 OpenCV BGR array\n",
        "    model,             # pretrained model\n",
        "    preprocess,        # torchvision transforms for the model\n",
        "    device,            # \"cuda\" or \"cpu\"\n",
        "    baseline=None,     # baseline tensor, default zero\n",
        "    n_steps=50,        # number of IG steps\n",
        "    target_layer=None  # if None: input‐IG; else: layer‐IG at this module\n",
        "):\n",
        "    \"\"\"Returns a [0,1] attribution map resized to the original H×W.\"\"\"\n",
        "    # 1) Preprocess\n",
        "    pil = Image.fromarray(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))\n",
        "    inp = preprocess(pil).unsqueeze(0).to(device)\n",
        "    if baseline is None:\n",
        "        baseline = torch.zeros_like(inp).to(device)\n",
        "\n",
        "    # 2) Predicted class\n",
        "    with torch.no_grad():\n",
        "        logits = model(inp)\n",
        "    cls = logits.argmax(dim=1).item()\n",
        "\n",
        "    # 3) Choose Captum explainer\n",
        "    if target_layer is None:\n",
        "        explainer = IntegratedGradients(model)\n",
        "        attributions = explainer.attribute(\n",
        "            inp,\n",
        "            baselines=baseline,\n",
        "            target=cls,\n",
        "            n_steps=n_steps\n",
        "        )  # shape (1,3,H,W)\n",
        "        # collapse to H×W\n",
        "        ig_map = attributions.squeeze().abs().sum(dim=0).cpu().numpy()\n",
        "    else:\n",
        "        explainer = LayerIntegratedGradients(model, target_layer)\n",
        "        attributions = explainer.attribute(\n",
        "            inp,\n",
        "            baselines=baseline,\n",
        "            target=cls,\n",
        "            n_steps=n_steps\n",
        "        )  # shape (1,C_l,H_l,W_l)\n",
        "        # collapse channels, average over channels\n",
        "        ig_map = attributions.squeeze().abs().mean(dim=0).cpu().numpy()\n",
        "\n",
        "    # 4) Normalize & resize\n",
        "    ig_map = (ig_map - ig_map.min())/(ig_map.max()-ig_map.min()+1e-8)\n",
        "    H, W = img_bgr.shape[:2]\n",
        "    ig_map = cv2.resize(ig_map, (W, H))\n",
        "\n",
        "    return ig_map\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: compare IG at early vs late ResNet layers\n",
        "early_layer = model.layer1[0]\n",
        "late_layer  = model.layer4[-1]\n",
        "\n",
        "ig_map_early = compute_ig(img_bgr, model, preprocess, device=device,\n",
        "                          baseline=None, n_steps=50,\n",
        "                          target_layer=early_layer)\n",
        "ig_map_late  = compute_ig(img_bgr, model, preprocess, device=device,\n",
        "                          baseline=None, n_steps=50,\n",
        "                          target_layer=late_layer)\n"
      ],
      "metadata": {
        "id": "xhqM6hGpO6CY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell: Insertion/Deletion AUC and IoU Scoring\n",
        "\n",
        "**`insertion_deletion(model, preprocess, img_bgr, heatmap, steps=50)`:**  \n",
        "1. Converts the input BGR image to a PIL image and creates a blurred version.  \n",
        "2. Flattens the heatmap to rank pixels by attribution strength.  \n",
        "3. Iteratively “inserts” top‐attributed pixels into the blurred image and records the model’s softmax score for the predicted class (`ins_scores`).  \n",
        "4. Iteratively “deletes” top‐attributed pixels from the original image (replacing them with blurred pixels) and records the model’s softmax score (`del_scores`).  \n",
        "5. Returns two lists: insertion AUC scores (`ins_scores`) and deletion AUC scores (`del_scores`)."
      ],
      "metadata": {
        "id": "ZV7uInLwPX8p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oD6td8oIjtg"
      },
      "outputs": [],
      "source": [
        "def insertion_deletion(model, preprocess, img_bgr, heatmap, steps=50):\n",
        "    import numpy as np, cv2\n",
        "    from PIL import Image\n",
        "\n",
        "    # 1) Prepare original & blurred\n",
        "    rgb      = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    pil = Image.fromarray(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))\n",
        "    inp = preprocess(pil).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        orig_out = model(inp)\n",
        "    cls      = orig_out.argmax(dim=1).item()\n",
        "    blur     = cv2.GaussianBlur(img_bgr, (51,51), 0)\n",
        "\n",
        "    # 2) Pixel ordering\n",
        "    h, w     = heatmap.shape\n",
        "    flat_idx = np.argsort(heatmap.flatten())[::-1]\n",
        "    total    = h * w\n",
        "\n",
        "    ins_scores, del_scores = [], []\n",
        "    for i in np.linspace(0, total, steps, endpoint=False, dtype=int):\n",
        "        mask = np.zeros(total, dtype=bool)\n",
        "        mask[flat_idx[:i]] = True\n",
        "        mask = mask.reshape(h, w)\n",
        "\n",
        "        # insertion\n",
        "        merged = blur.copy(); merged[mask] = img_bgr[mask]\n",
        "        pil_m  = Image.fromarray(cv2.cvtColor(merged, cv2.COLOR_BGR2RGB))\n",
        "        inp_i  = preprocess(pil_m).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            ins_scores.append(model(inp_i).softmax(1)[0, cls].item())\n",
        "\n",
        "        # deletion\n",
        "        deleted = img_bgr.copy(); deleted[mask] = blur[mask]\n",
        "        pil_d   = Image.fromarray(cv2.cvtColor(deleted, cv2.COLOR_BGR2RGB))\n",
        "        inp_d   = preprocess(pil_d).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            del_scores.append(model(inp_d).softmax(1)[0, cls].item())\n",
        "\n",
        "    return ins_scores, del_scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`iou_score(pred_mask, true_mask)`:**  \n",
        "Computes Intersection over Union (IoU) between a binary `pred_mask` (from thresholded heatmap) and `true_mask` (ground‐truth bounding regions).  \n",
        "- **Intersection:** `np.logical_and(pred_mask, true_mask).sum()`  \n",
        "- **Union:** `np.logical_or(pred_mask, true_mask).sum()`  \n",
        "- Returns `intersection / (union + 1e-8)` to avoid division by zero.\n",
        "\n",
        "These helper functions allow us to quantitatively evaluate how well each attribution method localizes the ground‐truth object (IoU) and how the predicted confidence changes as we insert or delete pixels (Insertion/Deletion AUC).  "
      ],
      "metadata": {
        "id": "LOz86urmPdg9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmTAW7Y3Imf5"
      },
      "outputs": [],
      "source": [
        "def iou_score(pred_mask, true_mask):\n",
        "    inter = np.logical_and(pred_mask, true_mask).sum()\n",
        "    uni   = np.logical_or(pred_mask, true_mask).sum()\n",
        "    return inter/(uni+1e-8)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "# Assuming `model`, `device`, `preprocess` already exist\n",
        "ig_explainer = IntegratedGradients(model)\n",
        "\n",
        "def compute_ig_map(\n",
        "    input_tensor,        # 4D tensor (1,C,H,W) already on device\n",
        "    target_class,        # int\n",
        "    original_shape,      # (H, W)\n",
        "    baseline=None,\n",
        "    n_steps=50\n",
        "):\n",
        "    if baseline is None:\n",
        "        baseline = torch.zeros_like(input_tensor)\n",
        "    attributions, _ = ig_explainer.attribute(\n",
        "        input_tensor,\n",
        "        baselines=baseline,\n",
        "        target=target_class,\n",
        "        n_steps=n_steps,\n",
        "        return_convergence_delta=True\n",
        "    )\n",
        "    sal = attributions.squeeze(0).abs().sum(dim=0)\n",
        "    sal = (sal - sal.min())/(sal.max()-sal.min()+1e-8)\n",
        "    sal_np = sal.detach().cpu().numpy()\n",
        "    H, W = original_shape\n",
        "    return cv2.resize(sal_np, (W, H))\n"
      ],
      "metadata": {
        "id": "hihrnhbBjg0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── Cell 2.1 (fixed): Define compute_lime_map ────────────────────────\n",
        "explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "def compute_lime_map(img_bgr, model, preprocess, device,\n",
        "                     num_samples=500,\n",
        "                     segmentation_kwargs={\"n_segments\":50, \"compactness\":10}):\n",
        "    import numpy as np, cv2, torch\n",
        "    from skimage.segmentation import slic\n",
        "    from torchvision.transforms import ToPILImage\n",
        "\n",
        "    # 1) BGR→RGB\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # 2) LIME requires a batch‐predict fn\n",
        "    def batch_predict(images):\n",
        "        batch = torch.stack([\n",
        "            preprocess(ToPILImage()(im)).to(device)\n",
        "            for im in images\n",
        "        ], dim=0)\n",
        "        with torch.no_grad():\n",
        "            probs = torch.softmax(model(batch), dim=1)\n",
        "        return probs.cpu().numpy()\n",
        "\n",
        "    # 3) Run LIME\n",
        "    lime_exp = explainer.explain_instance(\n",
        "        image=img_rgb,\n",
        "        classifier_fn=batch_predict,\n",
        "        top_labels=1,\n",
        "        num_samples=num_samples,\n",
        "        segmentation_fn=lambda x: slic(x, **segmentation_kwargs)\n",
        "    )\n",
        "\n",
        "    # 4) Correct unpack: first is the image, second is the H×W mask\n",
        "    _, seg_mask = lime_exp.get_image_and_mask(\n",
        "        lime_exp.top_labels[0],\n",
        "        positive_only=True,\n",
        "        num_features=segmentation_kwargs[\"n_segments\"],\n",
        "        hide_rest=False\n",
        "    )\n",
        "\n",
        "    # 5) Build continuous heatmap from segment weights\n",
        "    weights = dict(lime_exp.local_exp[lime_exp.top_labels[0]])\n",
        "    heatmap = np.zeros_like(seg_mask, dtype=float)\n",
        "    for seg_id, w in weights.items():\n",
        "        heatmap[seg_mask == seg_id] = w\n",
        "\n",
        "    # 6) Normalize to [0,1]\n",
        "    heatmap = np.clip(heatmap, 0, None)\n",
        "    heatmap = (heatmap - heatmap.min())/(heatmap.max()-heatmap.min()+1e-8)\n",
        "\n",
        "    return heatmap\n"
      ],
      "metadata": {
        "id": "v4OlrkD3C9tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Grad-CAM IoU (and Optional Insertion/Deletion) on VOC 2007\n",
        "\n",
        "The cell below iterates over the first 200 images in the Pascal VOC 2007 validation set (stored in `dataset`) and computes:\n",
        "\n",
        "1. **Grad-CAM heatmap** for each image using your pretrained ResNet50 model and the helper function `compute_cam(...)`.  \n",
        "2. **Binary mask** by thresholding the Grad-CAM heatmap at the 80th percentile (`best_p = 80`).  \n",
        "3. **IoU score** between this binary mask and the ground-truth bounding boxes (`iou_score(...)`), accumulating results into `cam_iou_list`.  \n",
        "4. *(Optional)* **Insertion/Deletion AUC** metrics for each Grad-CAM mask by calling `insertion_deletion(...)` with a coarser step size (10) and storing results in `cam_ins_list` and `cam_del_list`.\n",
        "\n",
        "**Prerequisites** (already defined in earlier cells):  \n",
        "- `model`, `device`, and `preprocess` (ResNet50 + its transforms)  \n",
        "- `dataset` (list of `(PIL.Image, [gt_boxes])` tuples)  \n",
        "- Helper functions: `compute_cam`, `iou_score`, `insertion_deletion`  \n",
        "\n",
        "After running this cell, you’ll have:  \n",
        "- `cam_iou_list`: IoU values (`mean ± std`) for the first 200 validation images  \n",
        "- *(Optional)* `cam_ins_list` and `cam_del_list`: insertion and deletion AUC scores for Grad-CAM masks  \n"
      ],
      "metadata": {
        "id": "BvQgN8xCQ-7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Cell A: Grad-CAM IoU + (optional) Insertion/Deletion on first 200 VOC images, early vs. late layers ──\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "# (Assumes `model`, `preprocess`, `dataset`, `compute_cam`, `iou_score`, and `insertion_deletion` are in scope.)\n",
        "\n",
        "# 1) Hyperparameters\n",
        "best_p     = 80      # percentile threshold to binarize a CAM heatmap\n",
        "max_images = 200     # how many images to process\n",
        "idel_steps = 10      # (coarser) steps for insertion/deletion\n",
        "\n",
        "# 2) Define early & late layers\n",
        "early_layer = model.layer1[0]\n",
        "late_layer  = model.layer4[-1]\n",
        "\n",
        "# 3) Storage lists for both layers\n",
        "cam_iou_early = []\n",
        "cam_ins_early = []\n",
        "cam_del_early = []\n",
        "\n",
        "cam_iou_late  = []\n",
        "cam_ins_late  = []\n",
        "cam_del_late  = []\n",
        "\n",
        "# 4) Loop\n",
        "start_time = time.time()\n",
        "for idx, (img, gt_boxes) in enumerate(dataset):\n",
        "    if idx >= max_images:\n",
        "        break\n",
        "\n",
        "    # Convert any type to BGR uint8\n",
        "    if isinstance(img, (str, Path)):\n",
        "        img_pil = Image.open(img).convert(\"RGB\")\n",
        "        img_np  = np.array(img_pil, dtype=np.uint8)\n",
        "        img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "    elif isinstance(img, Image.Image):\n",
        "        img_np  = np.array(img.convert(\"RGB\"), dtype=np.uint8)\n",
        "        img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "    elif isinstance(img, torch.Tensor):\n",
        "        arr = img.detach().cpu()\n",
        "        if arr.ndim == 4 and arr.shape[0] == 1:\n",
        "            arr = arr[0]\n",
        "        arr = (arr * 255.0).clamp(0,255).byte()\n",
        "        arr = arr.permute(1,2,0).cpu().numpy()\n",
        "        img_bgr = cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)\n",
        "    else:\n",
        "        raise RuntimeError(f\"Unrecognized image type: {type(img)}\")\n",
        "    H, W = img_bgr.shape[:2]\n",
        "\n",
        "    # — compute CAM & metrics for early layer —\n",
        "    cam_map_e, _  = compute_cam(img_bgr, model, preprocess,\n",
        "                                target_layer=early_layer, device=device)\n",
        "    thr_e        = np.percentile(cam_map_e, best_p)\n",
        "    mask_e       = (cam_map_e >= thr_e).astype(np.uint8)\n",
        "    true_mask    = np.zeros_like(mask_e)\n",
        "    for x1,y1,x2,y2 in gt_boxes:\n",
        "        true_mask[y1:y2, x1:x2] = 1\n",
        "    cam_iou_early.append(iou_score(mask_e, true_mask))\n",
        "    ins_e, del_e = insertion_deletion(model, preprocess, img_bgr, cam_map_e, steps=idel_steps)\n",
        "    cam_ins_early.append(np.trapz(ins_e) / len(ins_e))\n",
        "    cam_del_early.append(np.trapz(1.0 - np.array(del_e)) / len(del_e))\n",
        "\n",
        "    # — compute CAM & metrics for late layer —\n",
        "    cam_map_l, _  = compute_cam(img_bgr, model, preprocess,\n",
        "                                target_layer=late_layer, device=device)\n",
        "    thr_l        = np.percentile(cam_map_l, best_p)\n",
        "    mask_l       = (cam_map_l >= thr_l).astype(np.uint8)\n",
        "    true_mask    = np.zeros_like(mask_l)\n",
        "    for x1,y1,x2,y2 in gt_boxes:\n",
        "        true_mask[y1:y2, x1:x2] = 1\n",
        "    cam_iou_late.append(iou_score(mask_l, true_mask))\n",
        "    ins_l, del_l = insertion_deletion(model, preprocess, img_bgr, cam_map_l, steps=idel_steps)\n",
        "    cam_ins_late.append(np.trapz(ins_l) / len(ins_l))\n",
        "    cam_del_late.append(np.trapz(1.0 - np.array(del_l)) / len(del_l))\n",
        "\n",
        "# ── End loop ──\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "# 5) Print results\n",
        "print(f\"Processed {len(cam_iou_early)} images in {elapsed:.1f}s\\n\")\n",
        "\n",
        "print(\"EARLY LAYER (layer1[0])\")\n",
        "print(f\" IoU   = {np.mean(cam_iou_early):.3f} ± {np.std(cam_iou_early):.3f}\")\n",
        "print(f\" Ins AUC = {np.mean(cam_ins_early):.3f} ± {np.std(cam_ins_early):.3f}\")\n",
        "print(f\" Del AUC = {np.mean(cam_del_early):.3f} ± {np.std(cam_del_early):.3f}\\n\")\n",
        "\n",
        "print(\"LATE LAYER (layer4[-1])\")\n",
        "print(f\" IoU   = {np.mean(cam_iou_late):.3f} ± {np.std(cam_iou_late):.3f}\")\n",
        "print(f\" Ins AUC = {np.mean(cam_ins_late):.3f} ± {np.std(cam_ins_late):.3f}\")\n",
        "print(f\" Del AUC = {np.mean(cam_del_late):.3f} ± {np.std(cam_del_late):.3f}\")\n"
      ],
      "metadata": {
        "id": "E8gh8jD9jIon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── Cell A.1: Grad-CAM on First 400 VOC Images ──────────────────────────────\n",
        "import time, numpy as np\n",
        "from pathlib import Path\n",
        "import torch, cv2\n",
        "from PIL import Image\n",
        "\n",
        "# Hyperparameters\n",
        "best_p          = 80      # percentile for binarization\n",
        "idel_steps      = 10      # insertion/deletion steps\n",
        "max_images_full = 400     # process up to 400 images\n",
        "\n",
        "# Layers you compared in Cell A\n",
        "early_layer = model.layer1[0]\n",
        "late_layer  = model.layer4[-1]\n",
        "\n",
        "# Storage\n",
        "gc_iou_e, gc_ins_e, gc_del_e = [], [], []\n",
        "gc_iou_l, gc_ins_l, gc_del_l = [], [], []\n",
        "\n",
        "start = time.time()\n",
        "for idx, (img, gt_boxes) in enumerate(dataset):\n",
        "    if idx >= max_images_full:\n",
        "        break\n",
        "\n",
        "    # — Load into PIL.Image —\n",
        "    if isinstance(img, (str, Path)):\n",
        "        img_pil = Image.open(img).convert(\"RGB\")\n",
        "    elif isinstance(img, Image.Image):\n",
        "        img_pil = img.convert(\"RGB\")\n",
        "    else:\n",
        "        t = img.detach().cpu()\n",
        "        if t.ndim == 4 and t.size(0)==1:\n",
        "            t = t.squeeze(0)\n",
        "        arr = (t.permute(1,2,0).clamp(0,1)*255).byte().numpy()\n",
        "        img_pil = Image.fromarray(arr)\n",
        "\n",
        "    # — Preprocess & predict class —\n",
        "    inp = preprocess(img_pil).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        cls = int(model(inp).argmax(dim=1).item())\n",
        "\n",
        "    # — Grad-CAM at early layer —\n",
        "    # Replace grad_cam with compute_cam\n",
        "    cam_e, _ = compute_cam(cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR), model, preprocess, target_layer=early_layer, device=device)  # H×W float map\n",
        "    thr_e = np.percentile(cam_e, best_p)\n",
        "    mask_e = (cam_e >= thr_e).astype(np.uint8)\n",
        "    # build GT mask\n",
        "    true_e = np.zeros_like(mask_e)\n",
        "    H, W = mask_e.shape\n",
        "    for x1,y1,x2,y2 in gt_boxes:\n",
        "        true_e[y1:y2, x1:x2] = 1\n",
        "\n",
        "    gc_iou_e.append(iou_score(mask_e, true_e))\n",
        "    # insertion_deletion expects img_bgr and heatmap. Use img_bgr and cam_e\n",
        "    ins_e, del_e = insertion_deletion(model, preprocess, img_bgr=cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR), heatmap=cam_e, steps=idel_steps)\n",
        "    gc_ins_e.append(np.trapz(ins_e)/len(ins_e))\n",
        "    gc_del_e.append(np.trapz(1-np.array(del_e))/len(del_e))\n",
        "\n",
        "    # — Grad-CAM at late layer —\n",
        "    # Replace grad_cam with compute_cam\n",
        "    cam_l, _ = compute_cam(cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR), model, preprocess, target_layer=late_layer, device=device)\n",
        "    thr_l = np.percentile(cam_l, best_p)\n",
        "    mask_l = (cam_l >= thr_l).astype(np.uint8)\n",
        "    true_l = np.zeros_like(mask_l)\n",
        "    for x1,y1,x2,y2 in gt_boxes:\n",
        "        true_l[y1:y2, x1:x2] = 1\n",
        "\n",
        "    gc_iou_l.append(iou_score(mask_l, true_l))\n",
        "    # insertion_deletion expects img_bgr and heatmap. Use img_bgr and cam_l\n",
        "    ins_l, del_l = insertion_deletion(model, preprocess, img_bgr=cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR), heatmap=cam_l, steps=idel_steps)\n",
        "    gc_ins_l.append(np.trapz(ins_l)/len(ins_l))\n",
        "    gc_del_l.append(np.trapz(1-np.array(del_l))/len(del_l))\n",
        "\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"Expanded Grad-CAM on {len(gc_iou_e)} images × 400 cap in {elapsed:.1f}s\\n\")\n",
        "print(\"EARLY layer:\")\n",
        "print(f\" IoU   = {np.mean(gc_iou_e):.3f} ± {np.std(gc_iou_e):.3f}\")\n",
        "print(f\" Ins   = {np.mean(gc_ins_e):.3f} ± {np.std(gc_ins_e):.3f}\")\n",
        "print(f\" Del   = {np.mean(gc_del_e):.3f} ± {np.std(gc_del_e):.3f}\\n\")\n",
        "print(\"LATE layer:\")\n",
        "print(f\" IoU   = {np.mean(gc_iou_l):.3f} ± {np.std(gc_iou_l):.3f}\")\n",
        "print(f\" Ins   = {np.mean(gc_ins_l):.3f} ± {np.std(gc_ins_l):.3f}\")\n",
        "print(f\" Del   = {np.mean(gc_del_l):.3f} ± {np.std(gc_del_l):.3f}\")"
      ],
      "metadata": {
        "id": "dzkZqNt1A8th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Saliency IoU (and Optional Insertion/Deletion) on VOC 2007\n",
        "\n",
        "This cell processes the first `max_images` from the Pascal VOC 2007 validation set and computes:\n",
        "\n",
        "1. **Saliency Map** for each image using the pretrained ResNet50 model and the helper function `compute_saliency(...)`.  \n",
        "2. **Binary Mask** by thresholding the saliency map at the `best_p`-th percentile (`best_p = 80`).  \n",
        "3. **IoU Score** between that binary mask and the ground-truth bounding boxes (`iou_score(...)`), stored in `sal_iou_list`.  \n",
        "4. *(Optional)* **Insertion/Deletion AUC** metrics for each saliency mask via `insertion_deletion(...)` (using a coarser step size `idel_steps = 10`), stored in `sal_ins_list` and `sal_del_list`.\n",
        "\n",
        "**Assumes (defined in earlier cells):**  \n",
        "- `model`, `device`, `preprocess` (ResNet50 + its image transforms)  \n",
        "- `dataset` (list of `(PIL.Image, [gt_boxes])` tuples)  \n",
        "- Helper functions:  \n",
        "  - `compute_saliency(img_bgr, model, preprocess, device)` → returns a normalized saliency heatmap  \n",
        "  - `iou_score(pred_mask, true_mask)` → computes intersection-over-union  \n",
        "  - `insertion_deletion(model, preprocess, img_bgr, heatmap, steps)` → returns insertion/deletion curves  \n",
        "\n",
        "After running this cell, you will have:  \n",
        "- `sal_iou_list`: IoU values (`mean ± std`) for the first `max_images` images  \n",
        "- *(Optional)* `sal_ins_list` / `sal_del_list`: insertion and deletion AUC scores for saliency  \n",
        "\n"
      ],
      "metadata": {
        "id": "GTUU8khKRSND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Cell B: Saliency IoU + Insertion/Deletion on first max_images VOC images ──────\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "# (We assume the following variables/functions are already defined in previous cells:)\n",
        "#   - model            : a ResNet50 (or similar) in .eval() mode, on the proper device\n",
        "#   - preprocess       : the torchvision.transforms() for ResNet50\n",
        "#   - dataset          : list/iterable of (PILImage or Tensor or str/Path, gt_boxes) tuples\n",
        "#   - compute_saliency : function(img_bgr, model, preprocess, device) → H×W saliency map (0–1 float)\n",
        "#   - insertion_deletion(model, preprocess, img_bgr, heatmap, steps) → (ins_scores, del_scores)\n",
        "#   - iou_score        : function(pred_mask, true_mask) → intersection_over_union float\n",
        "#   - device           : \"cuda\" or \"cpu\"\n",
        "\n",
        "# 1) Hyperparameters\n",
        "best_p     = 80    # percentile threshold for turning a saliency map into a binary mask\n",
        "max_images = 200   # process at most this many images\n",
        "idel_steps = 10    # # of insertion/deletion steps (coarser = faster)\n",
        "\n",
        "# 2) Prepare storage\n",
        "sal_iou_list = []\n",
        "sal_ins_list = []\n",
        "sal_del_list = []\n",
        "\n",
        "# 3) Loop over first `max_images` entries\n",
        "start_time = time.time()\n",
        "\n",
        "for idx, (img, gt_boxes) in enumerate(dataset):\n",
        "    if idx >= max_images:\n",
        "        break\n",
        "\n",
        "    # ── 3.1) Convert whatever `img` is into a PIL.Image in \"RGB\" mode ───────────────\n",
        "    if isinstance(img, (str, Path)):\n",
        "        # If `img` is a filesystem path, open it with PIL and convert to RGB\n",
        "        img_pil = Image.open(img).convert(\"RGB\")\n",
        "    elif isinstance(img, Image.Image):\n",
        "        # Already a PIL.Image; just ensure it's RGB\n",
        "        img_pil = img.convert(\"RGB\")\n",
        "    elif isinstance(img, torch.Tensor):\n",
        "        # If it's a torch.Tensor, assume shape = (C,H,W) or (1,C,H,W)\n",
        "        t = img.detach().cpu()\n",
        "        if t.ndim == 4 and t.size(0) == 1:\n",
        "            t = t.squeeze(0)\n",
        "        # Now t is (C,H,W); permute → (H,W,C)\n",
        "        t = t.permute(1, 2, 0)\n",
        "        # If values are normalized floats (0–1), clamp&scale to [0–255]\n",
        "        arr = (t.clamp(0, 1) * 255.0).byte().numpy()\n",
        "        img_pil = Image.fromarray(arr)\n",
        "    else:\n",
        "        raise RuntimeError(f\"Unrecognized image type at index {idx}: {type(img)}\")\n",
        "\n",
        "    # ── 3.2) Convert PIL.Image → NumPy(RGB) → BGR for OpenCV ─────────────────────────\n",
        "    img_np = np.array(img_pil, dtype=np.uint8)             # shape = (H, W, 3), RGB order\n",
        "    img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)      # now shape = (H, W, 3), BGR order\n",
        "    H, W = img_bgr.shape[:2]\n",
        "\n",
        "    # ── 3.3) Compute Saliency Map using your helper function ─────────────────────────\n",
        "    sal_map = compute_saliency(img_bgr, model, preprocess, device)  # float32 H×W, values ∈ [0,1]\n",
        "\n",
        "    # ── 3.4) Threshold at the `best_p`‐percentile to get a binary mask ─────────────\n",
        "    thr = np.percentile(sal_map, best_p)\n",
        "    sal_mask = (sal_map >= thr).astype(np.uint8)  # H×W mask, 0/1\n",
        "\n",
        "    # ── 3.5) Build “true_mask” from GT boxes (list of [x1,y1,x2,y2]) ────────────────\n",
        "    true_mask = np.zeros_like(sal_mask, dtype=np.uint8)\n",
        "    for (x1, y1, x2, y2) in gt_boxes:\n",
        "        xx1 = max(0, min(W, x1))\n",
        "        yy1 = max(0, min(H, y1))\n",
        "        xx2 = max(0, min(W, x2))\n",
        "        yy2 = max(0, min(H, y2))\n",
        "        true_mask[yy1:yy2, xx1:xx2] = 1\n",
        "\n",
        "    # ── 3.6) Compute IoU between saliency‐mask and GT‐mask ─────────────────────────────\n",
        "    sal_iou_list.append(iou_score(sal_mask, true_mask))\n",
        "\n",
        "    # ── 3.7) Insertion/Deletion AUC (coarse, for speed) ─────────────────────────────\n",
        "    ins_vals, del_vals = insertion_deletion(model, preprocess, img_bgr, sal_map, steps=idel_steps)\n",
        "    sal_ins_list.append(np.trapz(ins_vals) / len(ins_vals))\n",
        "    sal_del_list.append(np.trapz(1.0 - np.array(del_vals)) / len(del_vals))\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "# 4) Print summary\n",
        "print(f\"Saliency: processed {len(sal_iou_list)} images in {elapsed:.1f} seconds\")\n",
        "print(f\"├─ Saliency IoU   = {np.mean(sal_iou_list):.3f} ± {np.std(sal_iou_list):.3f}\")\n",
        "print(f\"├─ Saliency Ins AUC = {np.mean(sal_ins_list):.3f} ± {np.std(sal_ins_list):.3f}\")\n",
        "print(f\"└─ Saliency Del AUC = {np.mean(sal_del_list):.3f} ± {np.std(sal_del_list):.3f}\")\n"
      ],
      "metadata": {
        "id": "69AxcrZ_x1mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── Cell B.3: Saliency on First 400 VOC Images ──────────────────────────────\n",
        "import time, numpy as np\n",
        "from pathlib import Path\n",
        "import torch, cv2\n",
        "from PIL import Image\n",
        "\n",
        "# Hyperparameters\n",
        "best_p          = 80      # percentile threshold\n",
        "idel_steps      = 10      # insertion/deletion steps\n",
        "max_images_full = 400     # cap at 400 images\n",
        "\n",
        "# Storage\n",
        "sal_iou_list, sal_ins_list, sal_del_list = [], [], []\n",
        "\n",
        "start = time.time()\n",
        "for idx, (img, gt_boxes) in enumerate(dataset):\n",
        "    if idx >= max_images_full:\n",
        "        break\n",
        "\n",
        "    # — Load as BGR for compute_saliency() —\n",
        "    if isinstance(img, (str, Path)):\n",
        "        img_pil = Image.open(img).convert(\"RGB\")\n",
        "    elif isinstance(img, Image.Image):\n",
        "        img_pil = img.convert(\"RGB\")\n",
        "    else:\n",
        "        t = img.detach().cpu()\n",
        "        if t.ndim == 4 and t.size(0)==1:\n",
        "            t = t.squeeze(0)\n",
        "        arr = (t.permute(1,2,0).clamp(0,1)*255).byte().numpy()\n",
        "        img_pil = Image.fromarray(arr)\n",
        "    img_np  = np.array(img_pil, np.uint8)\n",
        "    img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # — Compute vanilla saliency map —\n",
        "    sal_map = compute_saliency(img_bgr, model, preprocess, device)\n",
        "\n",
        "    # — Binarize →\n",
        "    thr       = np.percentile(sal_map, best_p)\n",
        "    sal_mask  = (sal_map >= thr).astype(np.uint8)\n",
        "\n",
        "    # — Build GT mask →\n",
        "    true_mask = np.zeros_like(sal_mask)\n",
        "    H, W = sal_mask.shape\n",
        "    for x1,y1,x2,y2 in gt_boxes:\n",
        "        true_mask[y1:y2, x1:x2] = 1\n",
        "\n",
        "    # — Metrics →\n",
        "    sal_iou_list.append(iou_score(sal_mask, true_mask))\n",
        "    ins_vals, del_vals = insertion_deletion(model, preprocess, img_bgr, sal_map, steps=idel_steps)\n",
        "    sal_ins_list.append(np.trapz(ins_vals)/len(ins_vals))\n",
        "    sal_del_list.append(np.trapz(1-np.array(del_vals))/len(del_vals))\n",
        "\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"Expanded Saliency on {len(sal_iou_list)} images × 400 cap in {elapsed:.1f}s\\n\")\n",
        "print(f\" IoU   = {np.mean(sal_iou_list):.3f} ± {np.std(sal_iou_list):.3f}\")\n",
        "print(f\" Ins   = {np.mean(sal_ins_list):.3f} ± {np.std(sal_ins_list):.3f}\")\n",
        "print(f\" Del   = {np.mean(sal_del_list):.3f} ± {np.std(sal_del_list):.3f}\")\n"
      ],
      "metadata": {
        "id": "3_6pLYTNC-ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── Cell B.1: SmoothGrad Wrapper for Your Saliency ───\n",
        "import numpy as np\n",
        "\n",
        "def compute_smoothgrad_saliency(img_bgr, model, preprocess, device,\n",
        "                                n_samples=25, noise_sigma=0.1):\n",
        "    \"\"\"\n",
        "    A SmoothGrad variant that calls your compute_saliency on noisy inputs.\n",
        "    Returns the average saliency map (shape H×W, values in [0,1]).\n",
        "    \"\"\"\n",
        "    grads = []\n",
        "    for _ in range(n_samples):\n",
        "        # 1) Add Gaussian noise in pixel space\n",
        "        noise = np.random.normal(0, noise_sigma, img_bgr.shape).astype(np.float32)\n",
        "        noisy_bgr = np.clip(img_bgr.astype(np.float32) + noise, 0, 255).astype(np.uint8)\n",
        "        # 2) Compute vanilla saliency on the noisy image\n",
        "        grads.append(compute_saliency(noisy_bgr, model, preprocess, device))\n",
        "    # 3) Mean of all saliency maps\n",
        "    return np.mean(grads, axis=0)\n"
      ],
      "metadata": {
        "id": "uvJYn8ZZA7vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── Cell B.2: SmoothGrad IoU & Insertion/Deletion Sweep ───\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "noise_levels = [0.05, 0.1, 0.2]\n",
        "sg_iou_means = []\n",
        "sg_ins_means = []\n",
        "sg_del_means = []\n",
        "\n",
        "for sigma in noise_levels:\n",
        "    iou_list, ins_list, del_list = [], [], []\n",
        "    for idx, (img, gt_boxes) in enumerate(dataset):\n",
        "        if idx >= max_images_full:  # reuse your max_images setting\n",
        "            break\n",
        "\n",
        "        # — convert to BGR as before —\n",
        "        img_pil = (Image.open(img).convert(\"RGB\")\n",
        "                   if isinstance(img, (str, Path))\n",
        "                   else (img.convert(\"RGB\") if isinstance(img, Image.Image)\n",
        "                         else None))\n",
        "        img_np = np.array(img_pil, dtype=np.uint8)\n",
        "        img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # 1) SmoothGrad saliency\n",
        "        sal = compute_smoothgrad_saliency(\n",
        "            img_bgr, model, preprocess, device,\n",
        "            n_samples=25, noise_sigma=sigma\n",
        "        )\n",
        "        # 2) Threshold → mask\n",
        "        thr = np.percentile(sal, best_p)\n",
        "        mask = (sal >= thr).astype(np.uint8)\n",
        "\n",
        "        # 3) Build true_mask (reuse your box loop)\n",
        "        H, W = mask.shape\n",
        "        true_mask = np.zeros_like(mask)\n",
        "        for (x1,y1,x2,y2) in gt_boxes:\n",
        "            true_mask[y1:y2, x1:x2] = 1\n",
        "\n",
        "        # 4) Metrics\n",
        "        iou_list.append(iou_score(mask, true_mask))\n",
        "        ins, dels = insertion_deletion(model, preprocess, img_bgr, sal, steps=idel_steps)\n",
        "        ins_list.append(np.trapz(ins)   / len(ins))\n",
        "        del_list.append(np.trapz(1.0 - np.array(dels)) / len(dels))\n",
        "\n",
        "    sg_iou_means.append(np.mean(iou_list))\n",
        "    sg_ins_means.append(np.mean(ins_list))\n",
        "    sg_del_means.append(np.mean(del_list))\n",
        "\n",
        "# 4) Plot results\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(noise_levels, sg_iou_means, marker='o', label='IoU')\n",
        "plt.plot(noise_levels, sg_ins_means, marker='o', label='Insertion AUC')\n",
        "plt.plot(noise_levels, sg_del_means, marker='o', label='Deletion AUC')\n",
        "plt.xlabel('SmoothGrad noise σ')\n",
        "plt.legend()\n",
        "plt.title('SmoothGrad Sensitivity')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6avfVidRBA6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Integrated Gradients IoU (and Optional Insertion/Deletion) on VOC 2007\n",
        "\n",
        "This cell processes the first `max_images` from the Pascal VOC 2007 validation set and computes:\n",
        "\n",
        "1. **Integrated Gradients (IG) Attribution Map** for each image using the pretrained ResNet50 model and the helper function `compute_ig_map(...)`.  \n",
        "2. **Binary Mask** by thresholding the IG map at the `best_p`-th percentile (`best_p = 80`).  \n",
        "3. **IoU Score** between that binary mask and the ground-truth bounding boxes (`iou_score(...)`), stored in `ig_iou_list`.  \n",
        "4. *(Optional)* **Insertion/Deletion AUC** metrics for each IG mask via `insertion_deletion(...)` (using a coarser step size `idel_steps = 10`), stored in `ig_ins_list` and `ig_del_list`.\n",
        "\n",
        "**Assumes (already defined earlier):**  \n",
        "- `model`, `device`, `preprocess` (ResNet50 + its transforms)  \n",
        "- `dataset` (list of `(PIL.Image or Tensor or Path, [gt_boxes])` tuples)  \n",
        "- Helper functions:  \n",
        "  • `compute_ig_map(input_tensor, target_class, original_shape, baseline=None, n_steps)` → returns a normalized IG heatmap ([0,1]) resized to (H,W)  \n",
        "  • `iou_score(pred_mask, true_mask)` → computes intersection‐over‐union  \n",
        "  • `insertion_deletion(model, preprocess, img_bgr, heatmap, steps)` → returns insertion/deletion score lists  \n",
        "\n",
        "After running this cell, you will have:  \n",
        "- `ig_iou_list`: IoU values (mean ± std) for the first `max_images` images  \n",
        "- *(Optional)* `ig_ins_list` and `ig_del_list`: insertion and deletion AUC scores for IG  \n"
      ],
      "metadata": {
        "id": "6DfpnUTeRrOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Cell C: IG IoU + Insertion/Deletion on first max_images VOC images (early vs. late layers) ──\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "# (Assumes `model`, `preprocess`, `dataset`, `compute_ig`, `insertion_deletion`, `iou_score`, and `device` are in scope.)\n",
        "\n",
        "# 1) Hyperparameters\n",
        "best_p     = 80    # percentile threshold for binarizing an IG map\n",
        "max_images = 200   # max images to process\n",
        "idel_steps = 10    # insertion/deletion steps\n",
        "ig_steps   = 50    # IG steps\n",
        "\n",
        "# 2) Early & late layers for layer‐IG\n",
        "early_layer = model.layer1[0]\n",
        "late_layer  = model.layer4[-1]\n",
        "\n",
        "# 3) Storage\n",
        "ig_iou_early = []; ig_ins_early = []; ig_del_early = []\n",
        "ig_iou_late  = []; ig_ins_late  = []; ig_del_late  = []\n",
        "\n",
        "# 4) Loop over dataset\n",
        "start_time = time.time()\n",
        "for idx, (img, gt_boxes) in enumerate(dataset):\n",
        "    if idx >= max_images:\n",
        "        break\n",
        "\n",
        "    # 4.1) Load image as PIL\n",
        "    if isinstance(img, (str, Path)):\n",
        "        img_pil = Image.open(img).convert(\"RGB\")\n",
        "    elif isinstance(img, Image.Image):\n",
        "        img_pil = img.convert(\"RGB\")\n",
        "    elif isinstance(img, torch.Tensor):\n",
        "        t = img.detach().cpu()\n",
        "        if t.ndim == 4 and t.size(0) == 1:\n",
        "            t = t.squeeze(0)\n",
        "        arr = (t.permute(1,2,0).clamp(0,1) * 255).byte().numpy()\n",
        "        img_pil = Image.fromarray(arr)\n",
        "    else:\n",
        "        raise RuntimeError(f\"Unrecognized image type: {type(img)}\")\n",
        "\n",
        "    # 4.2) To BGR for insertion/deletion\n",
        "    img_np  = np.array(img_pil, dtype=np.uint8)\n",
        "    img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "    H, W    = img_bgr.shape[:2]\n",
        "\n",
        "    # 4.3) Preprocess & predict class\n",
        "    inp = preprocess(img_pil).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        cls = int(model(inp).argmax(dim=1).item())\n",
        "\n",
        "    # 4.4) Compute IG at early layer\n",
        "    ig_map_e = compute_ig(\n",
        "        img_bgr, model, preprocess, device=device,\n",
        "        baseline=None, n_steps=ig_steps,\n",
        "        target_layer=early_layer\n",
        "    )\n",
        "    thr_e   = np.percentile(ig_map_e, best_p)\n",
        "    mask_e  = (ig_map_e >= thr_e).astype(np.uint8)\n",
        "    true_m  = np.zeros_like(mask_e)\n",
        "    for x1,y1,x2,y2 in gt_boxes:\n",
        "        true_m[y1:y2, x1:x2] = 1\n",
        "    ig_iou_early.append(iou_score(mask_e, true_m))\n",
        "    ins_e, del_e = insertion_deletion(model, preprocess, img_bgr, ig_map_e, steps=idel_steps)\n",
        "    ig_ins_early.append(np.trapz(ins_e) / len(ins_e))\n",
        "    ig_del_early.append(np.trapz(1.0 - np.array(del_e)) / len(del_e))\n",
        "\n",
        "    # 4.5) Compute IG at late layer\n",
        "    ig_map_l = compute_ig(\n",
        "        img_bgr, model, preprocess, device=device,\n",
        "        baseline=None, n_steps=ig_steps,\n",
        "        target_layer=late_layer\n",
        "    )\n",
        "    thr_l   = np.percentile(ig_map_l, best_p)\n",
        "    mask_l  = (ig_map_l >= thr_l).astype(np.uint8)\n",
        "    true_m  = np.zeros_like(mask_l)\n",
        "    for x1,y1,x2,y2 in gt_boxes:\n",
        "        true_m[y1:y2, x1:x2] = 1\n",
        "    ig_iou_late.append(iou_score(mask_l, true_m))\n",
        "    ins_l, del_l = insertion_deletion(model, preprocess, img_bgr, ig_map_l, steps=idel_steps)\n",
        "    ig_ins_late.append(np.trapz(ins_l) / len(ins_l))\n",
        "    ig_del_late.append(np.trapz(1.0 - np.array(del_l)) / len(del_l))\n",
        "\n",
        "# 5) Summary\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"Processed {len(ig_iou_early)} images in {elapsed:.1f}s\\n\")\n",
        "\n",
        "print(\"EARLY LAYER (layer1[0])\")\n",
        "print(f\" IG IoU   = {np.mean(ig_iou_early):.3f} ± {np.std(ig_iou_early):.3f}\")\n",
        "print(f\" Ins AUC  = {np.mean(ig_ins_early):.3f} ± {np.std(ig_ins_early):.3f}\")\n",
        "print(f\" Del AUC  = {np.mean(ig_del_early):.3f} ± {np.std(ig_del_early):.3f}\\n\")\n",
        "\n",
        "print(\"LATE LAYER (layer4[-1])\")\n",
        "print(f\" IG IoU   = {np.mean(ig_iou_late):.3f} ± {np.std(ig_iou_late):.3f}\")\n",
        "print(f\" Ins AUC  = {np.mean(ig_ins_late):.3f} ± {np.std(ig_ins_late):.3f}\")\n",
        "print(f\" Del AUC  = {np.mean(ig_del_late):.3f} ± {np.std(ig_del_late):.3f}\")\n"
      ],
      "metadata": {
        "id": "FXdk--iP1-oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── Cell C.1: Expanded IG on 400 VOC Images × 100 Steps ───\n",
        "import time, numpy as np\n",
        "from pathlib import Path\n",
        "import torch, cv2\n",
        "from PIL import Image\n",
        "\n",
        "# 1) New hyperparameters for the capped run\n",
        "best_p         = 80              # percentile for thresholding\n",
        "idel_steps     = 10              # insertion/deletion steps\n",
        "ig_steps_full  = 100             # IG steps\n",
        "max_images_full= 400             # cap at 400 images\n",
        "\n",
        "early_layer = model.layer1[0]\n",
        "late_layer  = model.layer4[-1]\n",
        "\n",
        "# 2) Storage for metrics\n",
        "full_iou_e, full_ins_e, full_del_e = [], [], []\n",
        "full_iou_l, full_ins_l, full_del_l = [], [], []\n",
        "\n",
        "# 3) Loop over the first 400 entries in your dataset\n",
        "start = time.time()\n",
        "for idx, (img, gt_boxes) in enumerate(dataset):\n",
        "    if idx >= max_images_full:\n",
        "        break\n",
        "\n",
        "    # — load PIL.Image as before —\n",
        "    if isinstance(img, (str, Path)):\n",
        "        img_pil = Image.open(img).convert(\"RGB\")\n",
        "    elif isinstance(img, Image.Image):\n",
        "        img_pil = img.convert(\"RGB\")\n",
        "    else:  # tensor or other\n",
        "        t = img.detach().cpu()\n",
        "        if t.ndim == 4 and t.size(0)==1:\n",
        "            t = t.squeeze(0)\n",
        "        arr = (t.permute(1,2,0).clamp(0,1)*255).byte().numpy()\n",
        "        img_pil = Image.fromarray(arr)\n",
        "\n",
        "    # — convert to BGR for ID curves —\n",
        "    img_np  = np.array(img_pil, np.uint8)\n",
        "    img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # — compute IG at early layer —\n",
        "    ig_e = compute_ig(\n",
        "        img_bgr, model, preprocess, device=device,\n",
        "        baseline=None, n_steps=ig_steps_full,\n",
        "        target_layer=early_layer\n",
        "    )\n",
        "    thr_e  = np.percentile(ig_e, best_p)\n",
        "    mask_e = (ig_e >= thr_e).astype(np.uint8)\n",
        "    true_e = np.zeros_like(mask_e)\n",
        "    for x1,y1,x2,y2 in gt_boxes:\n",
        "        true_e[y1:y2, x1:x2] = 1\n",
        "    full_iou_e.append(iou_score(mask_e, true_e))\n",
        "\n",
        "    ins_e, del_e = insertion_deletion(model, preprocess, img_bgr, ig_e, steps=idel_steps)\n",
        "    full_ins_e.append(np.trapz(ins_e) / len(ins_e))\n",
        "    full_del_e.append(np.trapz(1.0 - np.array(del_e)) / len(del_e))\n",
        "\n",
        "    # — compute IG at late layer —\n",
        "    ig_l = compute_ig(\n",
        "        img_bgr, model, preprocess, device=device,\n",
        "        baseline=None, n_steps=ig_steps_full,\n",
        "        target_layer=late_layer\n",
        "    )\n",
        "    thr_l  = np.percentile(ig_l, best_p)\n",
        "    mask_l = (ig_l >= thr_l).astype(np.uint8)\n",
        "    true_l = np.zeros_like(mask_l)\n",
        "    for x1,y1,x2,y2 in gt_boxes:\n",
        "        true_l[y1:y2, x1:x2] = 1\n",
        "    full_iou_l.append(iou_score(mask_l, true_l))\n",
        "\n",
        "    ins_l, del_l = insertion_deletion(model, preprocess, img_bgr, ig_l, steps=idel_steps)\n",
        "    full_ins_l.append(np.trapz(ins_l) / len(ins_l))\n",
        "    full_del_l.append(np.trapz(1.0 - np.array(del_l)) / len(del_l))\n",
        "\n",
        "# 4) Print summary\n",
        "elapsed = time.time() - start\n",
        "print(f\"Expanded IG run: {len(full_iou_e)} images × {ig_steps_full} steps in {elapsed:.1f}s\\n\")\n",
        "\n",
        "print(\"EARLY layer:\")\n",
        "print(f\" IoU     = {np.mean(full_iou_e):.3f} ± {np.std(full_iou_e):.3f}\")\n",
        "print(f\" Ins AUC = {np.mean(full_ins_e):.3f} ± {np.std(full_ins_e):.3f}\")\n",
        "print(f\" Del AUC = {np.mean(full_del_e):.3f} ± {np.std(full_del_e):.3f}\\n\")\n",
        "\n",
        "print(\"LATE layer:\")\n",
        "print(f\" IoU     = {np.mean(full_iou_l):.3f} ± {np.std(full_iou_l):.3f}\")\n",
        "print(f\" Ins AUC = {np.mean(full_ins_l):.3f} ± {np.std(full_ins_l):.3f}\")\n",
        "print(f\" Del AUC = {np.mean(full_del_l):.3f} ± {np.std(full_del_l):.3f}\")\n"
      ],
      "metadata": {
        "id": "7LpC615y8Mkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── Cell D.1: LIME Pilot on first 200 images ────────────────────\n",
        "import numpy as np\n",
        "\n",
        "best_p     = 80\n",
        "max_images = 200\n",
        "lime_iou, lime_ins, lime_del = [], [], []\n",
        "\n",
        "for idx, (img_pil, gt_boxes) in enumerate(dataset):\n",
        "    if idx >= max_images: break\n",
        "    rgb     = np.array(img_pil, dtype=np.uint8)\n",
        "    img_bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    lm = compute_lime_map(\n",
        "        img_bgr, model, preprocess, device,\n",
        "        num_samples=500, segmentation_kwargs={\"n_segments\":50,\"compactness\":10}\n",
        "    )\n",
        "    thr = np.percentile(lm, best_p)\n",
        "    mask = (lm >= thr).astype(np.uint8)\n",
        "\n",
        "    true_m = np.zeros_like(mask)\n",
        "    for x1,y1,x2,y2 in gt_boxes:\n",
        "        true_m[y1:y2, x1:x2] = 1\n",
        "\n",
        "    lime_iou.append(iou_score(mask, true_m))\n",
        "    ins, dels = insertion_deletion(model, preprocess, img_bgr, lm, steps=idel_steps)\n",
        "    lime_ins.append(np.trapz(ins)/len(ins))\n",
        "    lime_del.append(np.trapz(1-np.array(dels))/len(dels))\n",
        "\n",
        "print(f\"LIME Pilot IoU = {np.mean(lime_iou):.3f} ± {np.std(lime_iou):.3f}\")\n"
      ],
      "metadata": {
        "id": "ilKCv_xcDPXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"LIME Pilot Metrics (200 images):\")\n",
        "print(f\"  IoU     = {np.mean(lime_iou):.3f} ± {np.std(lime_iou):.3f}\")\n",
        "print(f\"  Ins AUC = {np.mean(lime_ins):.3f} ± {np.std(lime_ins):.3f}\")\n",
        "print(f\"  Del AUC = {np.mean(lime_del):.3f} ± {np.std(lime_del):.3f}\")\n"
      ],
      "metadata": {
        "id": "vCsc2BGIcmYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── Cell C.2: LIME Full on first 400 images ─────────────────────\n",
        "lime_full_iou, lime_full_ins, lime_full_del = [], [], []\n",
        "max_images_full = 400\n",
        "\n",
        "for idx, (img_pil, gt_boxes) in enumerate(dataset):\n",
        "    if idx >= max_images_full: break\n",
        "    rgb     = np.array(img_pil, dtype=np.uint8)\n",
        "    img_bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    lm = compute_lime_map(\n",
        "        img_bgr, model, preprocess, device,\n",
        "        num_samples=250, segmentation_kwargs={\"n_segments\":50,\"compactness\":10}\n",
        "    )\n",
        "    thr  = np.percentile(lm, best_p)\n",
        "    mask = (lm >= thr).astype(np.uint8)\n",
        "\n",
        "    true_m = np.zeros_like(mask)\n",
        "    for x1,y1,x2,y2 in gt_boxes:\n",
        "        true_m[y1:y2, x1:x2] = 1\n",
        "\n",
        "    lime_full_iou.append(iou_score(mask, true_m))\n",
        "    ins, dels = insertion_deletion(model, preprocess, img_bgr, lm, steps=idel_steps)\n",
        "    lime_full_ins.append(np.trapz(ins)/len(ins))\n",
        "    lime_full_del.append(np.trapz(1-np.array(dels))/len(dels))\n",
        "\n",
        "print(f\"LIME Full IoU = {np.mean(lime_full_iou):.3f} ± {np.std(lime_full_iou):.3f}\")\n"
      ],
      "metadata": {
        "id": "O6dn8K9FNMMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"LIME Full Metrics (400 images):\")\n",
        "print(f\"  IoU     = {np.mean(lime_full_iou):.3f} ± {np.std(lime_full_iou):.3f}\")\n",
        "print(f\"  Ins AUC = {np.mean(lime_full_ins):.3f} ± {np.std(lime_full_ins):.3f}\")\n",
        "print(f\"  Del AUC = {np.mean(lime_full_del):.3f} ± {np.std(lime_full_del):.3f}\")\n"
      ],
      "metadata": {
        "id": "MBk1FJZHcss7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell D: Consolidate Attribution Metrics into a DataFrame\n",
        "\n",
        "This cell gathers all of the previously computed metrics—IoU, insertion‐AUC, and deletion‐AUC—for Grad-CAM, Saliency, and Integrated Gradients into a single pandas DataFrame (`df`). Each column corresponds to one metric–method pair:\n",
        "\n",
        "- **cam_iou**, **cam_ins**, **cam_del**  \n",
        "- **sal_iou**, **sal_ins**, **sal_del**  \n",
        "- **ig_iou**,  **ig_ins**,  **ig_del**\n",
        "\n",
        "By assembling everything into `df`, you can easily preview, summarize, or export all nine metrics at once (e.g., with `df.head()`, `df.describe()`, or saving to CSV).\n"
      ],
      "metadata": {
        "id": "Sw7AvcPWR6cU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np # Import numpy to use np.min\n",
        "\n",
        "# 1. Get the minimum length among all lists\n",
        "# Assuming all lists are defined from previous cell executions\n",
        "list_lengths = [\n",
        "    len(cam_iou_early), len(cam_iou_late), len(cam_ins_early), len(cam_ins_late), len(cam_del_early), len(cam_del_late),\n",
        "    len(sal_iou_list), len(sal_ins_list), len(sal_del_list),\n",
        "    len(ig_iou_early), len(ig_iou_late), len(ig_ins_early), len(ig_ins_late), len(ig_del_early), len(ig_del_late)\n",
        "]\n",
        "min_len = np.min(list_lengths)\n",
        "\n",
        "print(f\"Truncating all lists to the minimum length found: {min_len}\")\n",
        "\n",
        "# 2. Truncate lists to the minimum length\n",
        "cam_iou_early_truncated = cam_iou_early[:min_len]\n",
        "cam_iou_late_truncated  = cam_iou_late[:min_len]\n",
        "cam_ins_early_truncated = cam_ins_early[:min_len]\n",
        "cam_ins_late_truncated  = cam_ins_late[:min_len]\n",
        "cam_del_early_truncated = cam_del_early[:min_len]\n",
        "cam_del_late_truncated  = cam_del_late[:min_len]\n",
        "\n",
        "sal_iou_list_truncated  = sal_iou_list[:min_len]\n",
        "sal_ins_list_truncated  = sal_ins_list[:min_len]\n",
        "sal_del_list_truncated  = sal_del_list[:min_len]\n",
        "\n",
        "ig_iou_early_truncated  = ig_iou_early[:min_len]\n",
        "ig_iou_late_truncated   = ig_iou_late[:min_len]\n",
        "ig_ins_early_truncated  = ig_ins_early[:min_len]\n",
        "ig_ins_late_truncated   = ig_ins_late[:min_len]\n",
        "ig_del_early_truncated  = ig_del_early[:min_len]\n",
        "ig_del_late_truncated   = ig_del_late[:min_len]\n",
        "\n",
        "\n",
        "# 3. Build pilot DataFrame using truncated lists\n",
        "df_pilot = pd.DataFrame({\n",
        "    # Grad-CAM pilot\n",
        "    \"cam_iou_early\": cam_iou_early_truncated,\n",
        "    \"cam_iou_late\":  cam_iou_late_truncated,\n",
        "    \"cam_ins_early\": cam_ins_early_truncated,\n",
        "    \"cam_ins_late\":  cam_ins_late_truncated,\n",
        "    \"cam_del_early\": cam_del_early_truncated,\n",
        "    \"cam_del_late\":  cam_del_late_truncated,\n",
        "    # Saliency pilot\n",
        "    \"sal_iou\":        sal_iou_list_truncated,\n",
        "    \"sal_ins\":        sal_ins_list_truncated,\n",
        "    \"sal_del\":        sal_del_list_truncated,\n",
        "    # IG pilot\n",
        "    \"ig_iou_early\":   ig_iou_early_truncated,\n",
        "    \"ig_iou_late\":    ig_iou_late_truncated,\n",
        "    \"ig_ins_early\":   ig_ins_early_truncated,\n",
        "    \"ig_ins_late\":    ig_ins_late_truncated,\n",
        "    \"ig_del_early\":   ig_del_early_truncated,\n",
        "    \"ig_del_late\":    ig_del_late_truncated,\n",
        "})\n",
        "\n",
        "# 4. Expand Saliency into early/late for consistency\n",
        "df_pilot[\"sal_iou_early\"] = df_pilot[\"sal_iou\"]\n",
        "df_pilot[\"sal_iou_late\"]  = df_pilot[\"sal_iou\"]\n",
        "df_pilot[\"sal_ins_early\"] = df_pilot[\"sal_ins\"]\n",
        "df_pilot[\"sal_ins_late\"]  = df_pilot[\"sal_ins\"]\n",
        "df_pilot[\"sal_del_early\"] = df_pilot[\"sal_del\"]\n",
        "df_pilot[\"sal_del_late\"]  = df_pilot[\"sal_del\"]\n",
        "\n",
        "# 5. Drop the generic sal_* columns\n",
        "df_pilot = df_pilot.drop(columns=[\"sal_iou\",\"sal_ins\",\"sal_del\"])\n",
        "\n",
        "# Assign to df as the original cell intended\n",
        "df = df_pilot\n",
        "\n",
        "# 6. Take a peek\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "UDBwDbo2L0t2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pilot[\"lime_iou_early\"] = lime_iou\n",
        "df_pilot[\"lime_ins_early\"] = lime_ins\n",
        "df_pilot[\"lime_del_early\"] = lime_del\n",
        "df_pilot[\"lime_iou_late\"]  = lime_iou\n",
        "df_pilot[\"lime_ins_late\"]  = lime_ins\n",
        "df_pilot[\"lime_del_late\"]  = lime_del\n"
      ],
      "metadata": {
        "id": "lW54RR86YNrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── Cell: Aggregate 400-image metrics into df_full ────────────────────────\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1) Sanity-check that all your 400-image lists exist and have the same length\n",
        "n = len(gc_iou_e)\n",
        "# Use the actual variable names for Saliency metrics on the full dataset\n",
        "lists = [\n",
        "    gc_iou_l, gc_ins_e, gc_ins_l, gc_del_e, gc_del_l,        # Grad-CAM\n",
        "    sal_iou_list, sal_ins_list, sal_del_list,                # Saliency (using actual names)\n",
        "    full_iou_e, full_iou_l, full_ins_e, full_ins_l, full_del_e, full_del_l  # IG\n",
        "]\n",
        "assert all(len(lst)==n for lst in lists), f\"List length mismatch: expected {n}\"\n",
        "\n",
        "# 2) Build the DataFrame\n",
        "df_full = pd.DataFrame({\n",
        "    # Grad-CAM\n",
        "    \"cam_iou_early\": gc_iou_e,\n",
        "    \"cam_iou_late\":  gc_iou_l,\n",
        "    \"cam_ins_early\": gc_ins_e,\n",
        "    \"cam_ins_late\":  gc_ins_l,\n",
        "    \"cam_del_early\": gc_del_e,\n",
        "    \"cam_del_late\":  gc_del_l,\n",
        "    # Saliency (same for early/late) - using actual names\n",
        "    \"sal_iou_early\": sal_iou_list,\n",
        "    \"sal_iou_late\":  sal_iou_list,\n",
        "    \"sal_ins_early\": sal_ins_list,\n",
        "    \"sal_ins_late\":  sal_ins_list,\n",
        "    \"sal_del_early\": sal_del_list,\n",
        "    \"sal_del_late\":  sal_del_list,\n",
        "    # Integrated Gradients\n",
        "    \"ig_iou_early\":  full_iou_e,\n",
        "    \"ig_iou_late\":   full_iou_l,\n",
        "    \"ig_ins_early\":  full_ins_e,\n",
        "    \"ig_ins_late\":   full_ins_l,\n",
        "    \"ig_del_early\":  full_del_e,\n",
        "    \"ig_del_late\":   full_del_l,\n",
        "})\n",
        "\n",
        "# 3) Quick sanity print\n",
        "print(f\"df_full: {df_full.shape[0]} rows × {df_full.shape[1]} columns\")\n",
        "df_full.head()"
      ],
      "metadata": {
        "id": "UedfBi-srlQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_full[\"lime_iou_early\"] = lime_full_iou\n",
        "df_full[\"lime_ins_early\"] = lime_full_ins\n",
        "df_full[\"lime_del_early\"] = lime_full_del\n",
        "df_full[\"lime_iou_late\"]  = lime_full_iou\n",
        "df_full[\"lime_ins_late\"]  = lime_full_ins\n",
        "df_full[\"lime_del_late\"]  = lime_full_del\n"
      ],
      "metadata": {
        "id": "niYAarljYYO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── Cell 21.1: Attribution Stability under Gaussian Noise (400 images) ────\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# 1) Settings\n",
        "noise_sigmas = [0.01, 0.05, 0.1]    # relative noise std-dev\n",
        "n_images     = 400                  # cap at 400 VOC images\n",
        "methods      = [\"Saliency\", \"Grad-CAM\", \"IG\"]\n",
        "stab_results = {m: [] for m in methods}\n",
        "\n",
        "# 2) Helper to get any method’s map\n",
        "# Assuming compute_saliency, compute_cam, compute_ig_map, model, preprocess, device are defined\n",
        "def get_map(img_bgr, method):\n",
        "    if method == \"Saliency\":\n",
        "        # Assuming compute_saliency takes img_bgr\n",
        "        return compute_saliency(img_bgr, model, preprocess, device)\n",
        "    elif method == \"Grad-CAM\":\n",
        "        # Replace compute_attribution with compute_cam\n",
        "        # compute_cam expects img_bgr, model, preprocess, device, and optional target_layer\n",
        "        # Assuming we use the default target_layer for this stability test\n",
        "        heatmap, _ = compute_cam(img_bgr, model, preprocess, device=device)\n",
        "        return heatmap # compute_cam returns heatmap and overlay, we need the heatmap\n",
        "    elif method == \"IG\":\n",
        "        # compute_ig_map expects a tensor and target_class\n",
        "        # Need to convert img_bgr to tensor and get target_class\n",
        "        img_pil = Image.fromarray(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))\n",
        "        inp = preprocess(img_pil).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            cls = int(model(inp).argmax(dim=1).item())\n",
        "        # Assuming compute_ig_map takes input tensor, target_class, original_shape, n_steps\n",
        "        return compute_ig_map(inp, cls, original_shape=img_bgr.shape[:2], n_steps=ig_steps_full)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown method: {method}\")\n",
        "\n",
        "# 3) Stability loop\n",
        "# Assuming dataset is available from previous cells and ig_steps_full is defined\n",
        "start = time.time() # Define start\n",
        "for sigma in noise_sigmas:\n",
        "    temp = {m: [] for m in methods}\n",
        "    # Iterate over the first n_images from the dataset\n",
        "    for idx, (img_pil, _) in enumerate(dataset):\n",
        "        if idx >= n_images: # Use n_images (400)\n",
        "            break\n",
        "\n",
        "        # PIL→RGB→BGR conversion is done inside get_map or should be done here?\n",
        "        # Let's keep the original structure and convert before calling get_map\n",
        "        if isinstance(img_pil, (str, Path)):\n",
        "            img_pil = Image.open(img_pil).convert(\"RGB\")\n",
        "        elif isinstance(img_pil, Image.Image):\n",
        "             img_pil = img_pil.convert(\"RGB\")\n",
        "        elif isinstance(img_pil, torch.Tensor):\n",
        "            t = img_pil.detach().cpu()\n",
        "            if t.ndim == 4 and t.size(0)==1:\n",
        "                t = t.squeeze(0)\n",
        "            arr = (t.permute(1,2,0).clamp(0,1) * 255).byte().numpy()\n",
        "            img_pil = Image.fromarray(arr)\n",
        "        else:\n",
        "             print(f\"Skipping unrecognized image type at index {idx}: {type(img_pil)}\")\n",
        "             continue\n",
        "\n",
        "        rgb     = np.array(img_pil, dtype=np.uint8)\n",
        "        img_bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        for m in methods:\n",
        "            # Ensure get_map returns a 2D heatmap for IoU calculation\n",
        "            # compute_saliency returns 2D, compute_cam returns 2D heatmap (first element), compute_ig_map returns 2D\n",
        "            try:\n",
        "                orig_map  = get_map(img_bgr, m)\n",
        "            except Exception as e:\n",
        "                print(f\"Error computing original map for method {m} on image {idx}: {e}\")\n",
        "                continue # Skip this image for this method\n",
        "\n",
        "            # add Gaussian noise in [0,255]\n",
        "            noise     = np.random.randn(*img_bgr.shape).astype(np.float32) * sigma * 255\n",
        "            noisy_bgr = np.clip(img_bgr.astype(np.float32) + noise, 0, 255).astype(np.uint8)\n",
        "\n",
        "            try:\n",
        "                noisy_map = get_map(noisy_bgr, m)\n",
        "            except Exception as e:\n",
        "                 print(f\"Error computing noisy map for method {m} on image {idx} with sigma {sigma}: {e}\")\n",
        "                 continue # Skip this image for this method\n",
        "\n",
        "\n",
        "            # IoU(original vs noisy) - need a function that computes IoU for two heatmaps\n",
        "            # Assuming iou_score function defined elsewhere is suitable for heatmaps\n",
        "            # Need to ensure heatmaps are in the correct format (e.g., binary masks or similar for iou_score)\n",
        "            # The original code used iou_score on masks, so we might need to threshold the heatmaps here\n",
        "            # Thresholding with percentile as done in other cells (e.g., FXdk--iP1-oW)\n",
        "            # Using a fixed percentile for simplicity in stability test, e.g., 80 as used elsewhere\n",
        "            best_p_stability = 80 # Using a threshold for binarization for IoU\n",
        "\n",
        "            if orig_map is not None and noisy_map is not None and orig_map.shape == noisy_map.shape:\n",
        "                try:\n",
        "                    # Threshold heatmaps to create binary masks for IoU\n",
        "                    thr_orig = np.percentile(orig_map, best_p_stability)\n",
        "                    mask_orig = (orig_map >= thr_orig).astype(np.uint8)\n",
        "\n",
        "                    thr_noisy = np.percentile(noisy_map, best_p_stability)\n",
        "                    mask_noisy = (noisy_map >= thr_noisy).astype(np.uint8)\n",
        "\n",
        "                    # Assuming iou_score takes two binary masks\n",
        "                    temp[m].append(iou_score(mask_noisy, mask_orig))\n",
        "                except Exception as e:\n",
        "                     print(f\"Error computing IoU for method {m} on image {idx} with sigma {sigma}: {e}\")\n",
        "            else:\n",
        "                print(f\"Skipping IoU for method {m} on image {idx} with sigma {sigma} due to invalid map shapes or None maps.\")\n",
        "\n",
        "\n",
        "    # record mean IoU for this sigma\n",
        "    for m in methods:\n",
        "        if temp[m]: # Only calculate mean if there are valid IoU scores\n",
        "            stab_results[m].append(np.mean(temp[m]))\n",
        "        else:\n",
        "            stab_results[m].append(np.nan) # Append NaN if no valid scores for this sigma and method\n",
        "\n",
        "# 4) Plot\n",
        "plt.figure(figsize=(6,4), dpi=150)\n",
        "for m in methods:\n",
        "    # Only plot if there are valid results for this method\n",
        "    if stab_results[m]:\n",
        "        plt.plot(noise_sigmas[:len(stab_results[m])], stab_results[m], marker=\"o\", label=m) # Ensure noise_sigmas matches results length\n",
        "\n",
        "plt.xlabel(\"Input noise σ\")\n",
        "plt.ylabel(\"Mean IoU (orig vs noisy mask)\")\n",
        "plt.ylim(0,1)\n",
        "plt.title(\"Attribution Stability under Gaussian Noise (400 images)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "elapsed = time.time() - start # Calculate elapsed time after the loop and before plotting\n",
        "print(f\"\\nStability analysis on {n_images} images done in {elapsed:.1f}s\")"
      ],
      "metadata": {
        "id": "GZo5KzPzuHJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell E: Plot Comparative Bar Charts for Attribution Metrics\n",
        "\n",
        "This cell creates three side‐by‐side bar charts—one each for IoU, Insertion AUC, and Deletion AUC—to visually compare the three attribution methods (Grad-CAM, Saliency, and Integrated Gradients) over the first 200 VOC validation images.  \n",
        "\n",
        "- **Labels**: `[\"CAM\", \"Saliency\", \"IG\"]`  \n",
        "- **Metric Groups**:  \n",
        "  - `(\"cam_iou\",  \"sal_iou\",  \"ig_iou\")`   → Intersection-over-Union scores  \n",
        "  - `(\"cam_ins\", \"sal_ins\", \"ig_ins\")`   → Insertion AUC (higher = better)  \n",
        "  - `(\"cam_del\", \"sal_del\", \"ig_del\")`   → Deletion AUC (higher = better)  \n",
        "\n",
        "For each subplot:  \n",
        "1. Compute the mean and standard deviation of the three methods for that metric.  \n",
        "2. Draw a bar chart with error bars (± 1 std) and set y-axis limits to [0, 1].  \n",
        "3. Title each subplot accordingly (“IoU”, “Ins AUC”, “Del AUC”).  \n",
        "\n",
        "Finally, the figure is given an overall title (“Comparison of Attribution Methods (200 VOC images)”), and the DataFrame and figure can be optionally saved to disk.  \n"
      ],
      "metadata": {
        "id": "PYsBde3NSLfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "labels = [\"CAM\", \"Saliency\", \"IG\", \"LIME\"]\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "metrics = [\"iou\", \"ins\", \"del\"]\n",
        "titles  = [\"IoU\", \"Ins AUC\", \"Del AUC\"]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5), dpi=150)\n",
        "\n",
        "for ax, metric, title in zip(axes, metrics, titles):\n",
        "    # Include LIME columns in the lists for calculating means and errors\n",
        "    early_cols = [f\"cam_{metric}_early\", f\"sal_{metric}_early\", f\"ig_{metric}_early\", f\"lime_{metric}_early\"]\n",
        "    late_cols  = [f\"cam_{metric}_late\",  f\"sal_{metric}_late\",  f\"ig_{metric}_late\",  f\"lime_{metric}_late\"]\n",
        "\n",
        "    # Ensure df is defined and contains these columns. Assuming df is df_pilot or df_full\n",
        "    # based on previous cells. Using df directly as it was the last assigned DataFrame.\n",
        "    means_early = df[early_cols].mean().values\n",
        "    errs_early  = df[early_cols].std().values\n",
        "    means_late  = df[late_cols].mean().values\n",
        "    errs_late   = df[late_cols].std().values\n",
        "\n",
        "    ax.bar(x - width/2, means_early, width, yerr=errs_early, capsize=5, label=\"Early\")\n",
        "    ax.bar(x + width/2, means_late,  width, yerr=errs_late,  capsize=5, label=\"Late\")\n",
        "\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.set_ylim(0, 1.0)\n",
        "    ax.set_title(title)\n",
        "    if metric == \"iou\":\n",
        "        ax.set_ylabel(\"Score\")\n",
        "    ax.legend()\n",
        "\n",
        "plt.suptitle(\"Early vs. Late Layer Comparison of Attribution Metrics\", y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sguf4N3cL5WJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Common labels\n",
        "methods = [\"CAM\", \"Saliency\", \"IG\"]\n",
        "x = np.arange(len(methods))\n",
        "width = 0.35\n",
        "\n",
        "# Define the two runs and their DataFrames\n",
        "runs = [\n",
        "    (\"Pilot (200 images)\", df_pilot),\n",
        "    (\"Full  (400 images)\", df_full)\n",
        "]\n",
        "\n",
        "metrics = [\n",
        "    (\"iou\", \"IoU\"),\n",
        "    (\"ins\", \"Insertion AUC\"),\n",
        "    (\"del\", \"Deletion AUC\")\n",
        "]\n",
        "\n",
        "fig, axes = plt.subplots(\n",
        "    nrows=len(runs), ncols=len(metrics),\n",
        "    figsize=(15, 8), dpi=150\n",
        ")\n",
        "\n",
        "for i, (run_label, df_run) in enumerate(runs):\n",
        "    for j, (metric, title) in enumerate(metrics):\n",
        "        ax = axes[i, j]\n",
        "\n",
        "        # collect early/late columns\n",
        "        early_cols = [f\"cam_{metric}_early\", f\"sal_{metric}_early\", f\"ig_{metric}_early\"]\n",
        "        late_cols  = [f\"cam_{metric}_late\",  f\"sal_{metric}_late\",  f\"ig_{metric}_late\"]\n",
        "\n",
        "        means_early = df_run[early_cols].mean().values\n",
        "        errs_early  = df_run[early_cols].std().values\n",
        "        means_late  = df_run[late_cols].mean().values\n",
        "        errs_late   = df_run[late_cols].std().values\n",
        "\n",
        "        ax.bar(x - width/2, means_early, width, yerr=errs_early, capsize=5, label=\"Early\")\n",
        "        ax.bar(x + width/2, means_late,  width, yerr=errs_late,  capsize=5, label=\"Late\")\n",
        "\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(methods)\n",
        "        ax.set_ylim(0, 1.0)\n",
        "        ax.set_title(f\"{title}\\n{run_label}\")\n",
        "        if j == 0:\n",
        "            ax.set_ylabel(\"Score\")\n",
        "        # only add legend once\n",
        "        if i == 0 and j == len(metrics) - 1:\n",
        "            ax.legend(loc=\"upper right\")\n",
        "\n",
        "plt.suptitle(\"Attribution Metrics: Pilot vs. Full Runs\", y=1.04)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QF5Wz-7sslyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell D: Build 5×4 “Survey Examples” Grid\n",
        "\n",
        "This cell selects five representative VOC validation images (by index), runs the model to predict each image’s top class, draws the ground‐truth bounding boxes, and then visualizes all three attribution methods side‐by‐side. Specifically:  \n",
        "1. **Choose indices** (`idx_list = [10, 88, 122, 450, 1200]`).  \n",
        "2. **Loop** over each index:  \n",
        "   - Load the PIL image and its GT boxes.  \n",
        "   - Convert to BGR for OpenCV, preprocess for ResNet50, and run a forward pass to get the top predicted class name.  \n",
        "   - Overlay GT bounding boxes in red on a copy of the original.  \n",
        "   - Compute the three attribution heatmaps (Grad-CAM, Saliency, IG), resize them back to the original image size, and overlay each on the RGB image using a semi-transparent “jet” colormap.  \n",
        "3. **Arrange** these five images in a 5-row × 4-column grid:  \n",
        "   - Column 1: “Original + GT boxes” with the predicted class label.  \n",
        "   - Column 2: Grad-CAM overlay.  \n",
        "   - Column 3: Saliency overlay.  \n",
        "   - Column 4: Integrated Gradients overlay.  \n",
        "4. **Add a super‐title** (“Survey Examples: Original / Grad-CAM / Saliency / IG (5 Images)”).  \n",
        "5. **Save** the resulting figure as `survey_examples_with_labels_and_boxes.png` for use in the questionnaire.  \n"
      ],
      "metadata": {
        "id": "1zgpYezfSf9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Cell S1: Survey Stage 1 – Overlays Only (group by example) ─────────────\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "idx_list = [10, 88, 122, 450, 1200]\n",
        "late_layer = model.layer4[-1]\n",
        "n_rows, n_cols = len(idx_list), 4\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4 * n_rows), dpi=100)\n",
        "\n",
        "# Letters A–T for 5×4 grid\n",
        "letters = [chr(ord('A') + i) for i in range(n_rows * n_cols)]\n",
        "titles  = [\"Original+GT\", \"CAM Late\", \"Saliency\", \"IG Late\"]\n",
        "\n",
        "for r, img_idx in enumerate(idx_list):\n",
        "    # Load and draw GT\n",
        "    img_pil, gt_boxes = dataset[img_idx]\n",
        "    img_rgb = np.array(img_pil.convert(\"RGB\"))\n",
        "    viz = img_rgb.copy()\n",
        "    for (x1,y1,x2,y2) in gt_boxes:\n",
        "        cv2.rectangle(viz, (x1,y1), (x2,y2), (255,0,0), 2)\n",
        "\n",
        "    # Compute overlays\n",
        "    img_bgr  = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n",
        "    cam_l, _ = compute_cam(img_bgr, model, preprocess,\n",
        "                           target_layer=late_layer, device=device)\n",
        "    sal_map  = compute_saliency(img_bgr, model, preprocess, device)\n",
        "    ig_l     = compute_ig(img_bgr, model, preprocess, device,\n",
        "                          n_steps=20, target_layer=late_layer)\n",
        "\n",
        "    overlays = [viz, cam_l, sal_map, ig_l]\n",
        "\n",
        "    for c in range(n_cols):\n",
        "        ax = axes[r, c]\n",
        "        # Letter annotation\n",
        "        ax.text(0.02, 0.90, letters[r*n_cols + c],\n",
        "                transform=ax.transAxes,\n",
        "                fontsize=12, fontweight='bold', color='white')\n",
        "        if c == 0:\n",
        "            im = ax.imshow(overlays[c], vmin=0, vmax=255)\n",
        "        else:\n",
        "            ax.imshow(viz, vmin=0, vmax=255)\n",
        "            im = ax.imshow(overlays[c], cmap='jet', alpha=0.6,\n",
        "                           vmin=0, vmax=1)\n",
        "        ax.set_title(f\"{titles[c]}\", fontsize=10)\n",
        "        ax.axis('off')\n",
        "\n",
        "# Shared colorbar for columns 1–3\n",
        "cbar = fig.colorbar(im, ax=axes[:,1:], orientation='vertical',\n",
        "                    fraction=0.02, pad=0.01, label='Attribution intensity')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.95, hspace=0.3)\n",
        "plt.suptitle(\"Stage 1: Overlays Only (Late-Layer Comparisons)\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "fig.savefig(\"survey_stage1_overlays.png\", dpi=200, bbox_inches=\"tight\")\n",
        "print(\"✅ Saved survey_stage1_overlays.png\")\n"
      ],
      "metadata": {
        "id": "S-KUno-U9urF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Cell S1: Survey Stage 1 – Overlays Only (group by example, with LIME) ──\n",
        "import matplotlib.pyplot as plt, numpy as np, cv2\n",
        "from PIL import Image\n",
        "\n",
        "idx_list   = [10, 88, 122, 450, 1200]\n",
        "late_layer = model.layer4[-1]\n",
        "n_rows, n_cols = len(idx_list), 5\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 4 * n_rows), dpi=100)\n",
        "\n",
        "letters = [chr(ord('A') + i) for i in range(n_rows * n_cols)]\n",
        "titles  = [\"Original+GT\", \"CAM Late\", \"Saliency\", \"IG Late\", \"LIME\"]\n",
        "\n",
        "for r, img_idx in enumerate(idx_list):\n",
        "    # ── 0) load image & GT boxes ───────────────────────────────────────────\n",
        "    img_pil, gt_boxes = dataset[img_idx]\n",
        "    img_rgb = np.array(img_pil.convert(\"RGB\"))\n",
        "    viz     = img_rgb.copy()\n",
        "    for (x1, y1, x2, y2) in gt_boxes:\n",
        "        cv2.rectangle(viz, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "\n",
        "    # ── 0.1) predict class & name for annotation ──────────────────────────\n",
        "    with torch.no_grad():\n",
        "        cls_idx  = int(model(preprocess(img_pil).unsqueeze(0).to(device)).argmax(1))\n",
        "        cls_name = weights.meta[\"categories\"][cls_idx]\n",
        "\n",
        "    # ── 1) compute maps ───────────────────────────────────────────────────\n",
        "    img_bgr  = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n",
        "    cam_l, _ = compute_cam(img_bgr, model, preprocess, target_layer=late_layer, device=device)\n",
        "    sal_map  = compute_saliency(img_bgr, model, preprocess, device)\n",
        "    ig_l     = compute_ig(img_bgr, model, preprocess, device, n_steps=20, target_layer=late_layer)\n",
        "    lime_map = compute_lime_map(img_bgr, model, preprocess, device)\n",
        "\n",
        "    overlays = [viz, cam_l, sal_map, ig_l, lime_map]\n",
        "\n",
        "    # ── 2) plot each column ───────────────────────────────────────────────\n",
        "    for c in range(n_cols):\n",
        "        ax = axes[r, c]\n",
        "        ax.text(0.02, 0.90, letters[r*n_cols + c],\n",
        "                transform=ax.transAxes, fontsize=12, fontweight='bold', color='white')\n",
        "\n",
        "        if c == 0:                                          # original + GT column\n",
        "            ax.imshow(overlays[c], vmin=0, vmax=255)\n",
        "            ax.set_title(f\"{titles[c]}\\nPred: {cls_name}\", fontsize=10)\n",
        "        else:                                               # attribution overlays\n",
        "            ax.imshow(viz, vmin=0, vmax=255)\n",
        "            im = ax.imshow(overlays[c], cmap='jet', alpha=0.5, vmin=0, vmax=1)\n",
        "            ax.set_title(titles[c], fontsize=10)\n",
        "        ax.axis('off')\n",
        "\n",
        "# shared color-bar and layout tweaks (unchanged)\n",
        "cbar = fig.colorbar(im, ax=axes[:,1:], orientation='vertical',\n",
        "                    fraction=0.02, pad=0.01, label='Attribution intensity')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.95, hspace=0.3)\n",
        "plt.suptitle(\"Stage 1: Overlays Only (Late-Layer + LIME)\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "fig.savefig(\"survey_stage1_overlays_lime.png\", dpi=200, bbox_inches=\"tight\")\n",
        "print(\"✅ Saved survey_stage1_overlays_lime.png\")\n"
      ],
      "metadata": {
        "id": "H44QzekNjVS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Cell S2′: Survey Stage 2 – Originals + Early vs. Late (CAM & IG) ─────────────\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "idx_list    = [10, 88, 122, 450, 1200]\n",
        "early_layer = model.layer1[0]\n",
        "late_layer  = model.layer4[-1]\n",
        "methods     = [(\"CAM\", compute_cam), (\"IG\", compute_ig)]\n",
        "\n",
        "n_rows = len(idx_list)\n",
        "n_cols = 1 + len(methods)*2   # 1 for original, 2 layers × 2 methods = 5\n",
        "fig, axes = plt.subplots(n_rows, n_cols,\n",
        "                         figsize=(18, 4 * n_rows),\n",
        "                         dpi=100)\n",
        "\n",
        "# Generate letter annotations A, B, C...\n",
        "total_panels = n_rows * n_cols\n",
        "letters = [chr(ord('A') + i) for i in range(total_panels)]\n",
        "\n",
        "for r, img_idx in enumerate(idx_list):\n",
        "    # 0) Load image + GT boxes\n",
        "    img_pil, gt_boxes = dataset[img_idx]\n",
        "    img_rgb = np.array(img_pil.convert(\"RGB\"))\n",
        "    viz     = img_rgb.copy()\n",
        "    for x1,y1,x2,y2 in gt_boxes:\n",
        "        cv2.rectangle(viz, (x1,y1), (x2,y2), (255,0,0), 2)\n",
        "\n",
        "    # 1) Predict class (optional if you want title)\n",
        "    inp = preprocess(img_pil).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        cls_idx  = int(model(inp).argmax(dim=1).item())\n",
        "        cls_name = weights.meta[\"categories\"][cls_idx]\n",
        "\n",
        "    # 2) Compute heatmaps\n",
        "    img_bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n",
        "    cams    = []\n",
        "    igs     = []\n",
        "    for name, fn in methods:\n",
        "        if name == \"CAM\":\n",
        "            cam_e, _ = fn(img_bgr, model, preprocess,\n",
        "                          target_layer=early_layer, device=device)\n",
        "            cam_l, _ = fn(img_bgr, model, preprocess,\n",
        "                          target_layer=late_layer,  device=device)\n",
        "            cams.extend([cam_e, cam_l])\n",
        "        else:  # IG\n",
        "            ig_e = fn(img_bgr, model, preprocess, device,\n",
        "                      baseline=None, n_steps=20,\n",
        "                      target_layer=early_layer)\n",
        "            ig_l = fn(img_bgr, model, preprocess, device,\n",
        "                      baseline=None, n_steps=20,\n",
        "                      target_layer=late_layer)\n",
        "            igs.extend([ig_e, ig_l])\n",
        "\n",
        "    heatmaps = cams + igs  # order: CAM early, CAM late, IG early, IG late\n",
        "\n",
        "    # 3) Plot original + GT in col 0\n",
        "    ax = axes[r, 0]\n",
        "    idx_letter = r*n_cols + 0\n",
        "    ax.text(0.02, 0.90, letters[idx_letter],\n",
        "            transform=ax.transAxes,\n",
        "            fontsize=12, fontweight='bold', color='white')\n",
        "    ax.imshow(viz, vmin=0, vmax=255)\n",
        "    ax.set_title(f\"Original + GT\\nIdx {img_idx}\", fontsize=10)\n",
        "    ax.axis('off')\n",
        "\n",
        "    # 4) Plot heatmap columns\n",
        "    for c in range(1, n_cols):\n",
        "        ax = axes[r, c]\n",
        "        idx_letter = r*n_cols + c\n",
        "        ax.text(0.02, 0.90, letters[idx_letter],\n",
        "                transform=ax.transAxes,\n",
        "                fontsize=12, fontweight='bold', color='white')\n",
        "\n",
        "        hm = heatmaps[c-1]\n",
        "        im = ax.imshow(hm, cmap='jet', vmin=0, vmax=1)\n",
        "        method_idx = (c-1) // 2\n",
        "        layer_lbl  = \"Early\" if (c-1) % 2 == 0 else \"Late\"\n",
        "        ax.set_title(f\"{methods[method_idx][0]} {layer_lbl}\", fontsize=10)\n",
        "        ax.axis('off')\n",
        "\n",
        "# 5) Shared colorbar for heatmaps\n",
        "cbar = fig.colorbar(im, ax=axes[:, 1:], orientation='vertical',\n",
        "                    fraction=0.02, pad=0.01, label='Attribution intensity')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.94, hspace=0.3)\n",
        "plt.suptitle(\"Stage 2: Originals + CAM/IG Early vs. Late\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "fig.savefig(\"survey_stage2_with_originals.png\", dpi=200, bbox_inches=\"tight\")\n",
        "print(\"✅ Saved as survey_stage2_with_originals.png\")\n"
      ],
      "metadata": {
        "id": "SJnpj4K1-e66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Cell S2: Survey Stage 2 – Originals + CAM, IG & LIME (standalone heatmaps) ──\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "idx_list    = [10, 88, 122, 450, 1200]\n",
        "early_layer = model.layer1[0]\n",
        "late_layer  = model.layer4[-1]\n",
        "methods     = [(\"CAM\", compute_cam), (\"IG\", compute_ig), (\"LIME\", compute_lime_map)]\n",
        "\n",
        "n_rows = len(idx_list)\n",
        "n_cols = 1 + 2 + 2 + 1   # 1 original, 2 CAM, 2 IG, 1 LIME = 6\n",
        "fig, axes = plt.subplots(n_rows, n_cols,\n",
        "                         figsize=(18, 4 * n_rows),\n",
        "                         dpi=100)\n",
        "\n",
        "# Letter labels A–Z…\n",
        "letters = [chr(ord('A') + i) for i in range(n_rows * n_cols)]\n",
        "titles  = [\"Original+GT\",\n",
        "           \"CAM Early\", \"CAM Late\",\n",
        "           \"IG Early\",  \"IG Late\",\n",
        "           \"LIME\"]\n",
        "\n",
        "for r, img_idx in enumerate(idx_list):\n",
        "    img_pil, gt_boxes = dataset[img_idx]\n",
        "    img_rgb = np.array(img_pil.convert(\"RGB\"))\n",
        "    viz     = img_rgb.copy()\n",
        "    for (x1,y1,x2,y2) in gt_boxes:\n",
        "        cv2.rectangle(viz, (x1,y1), (x2,y2), (255,0,0), 2)\n",
        "\n",
        "    # 1) Original + GT\n",
        "    ax = axes[r, 0]\n",
        "    ax.text(0.02, 0.90, letters[r*n_cols + 0],\n",
        "            transform=ax.transAxes,\n",
        "            fontsize=12, fontweight='bold', color='white')\n",
        "    ax.imshow(viz, vmin=0, vmax=255)\n",
        "    ax.set_title(titles[0], fontsize=10)\n",
        "    ax.axis('off')\n",
        "\n",
        "    # 2) Compute and plot each attribution map\n",
        "    heatmaps = []\n",
        "    # 2a) CAM\n",
        "    img_bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n",
        "    cam_e, _ = compute_cam(img_bgr, model, preprocess,\n",
        "                          target_layer=early_layer, device=device)\n",
        "    cam_l, _ = compute_cam(img_bgr, model, preprocess,\n",
        "                          target_layer=late_layer,  device=device)\n",
        "    heatmaps += [cam_e, cam_l]\n",
        "    # 2b) IG\n",
        "    ig_e = compute_ig(img_bgr, model, preprocess,\n",
        "                      device=device, baseline=None,\n",
        "                      n_steps=20, target_layer=early_layer)\n",
        "    ig_l = compute_ig(img_bgr, model, preprocess,\n",
        "                      device=device, baseline=None,\n",
        "                      n_steps=20, target_layer=late_layer)\n",
        "    heatmaps += [ig_e, ig_l]\n",
        "    # 2c) LIME\n",
        "    lime_map = compute_lime_map(img_bgr, model, preprocess, device)\n",
        "    heatmaps.append(lime_map)\n",
        "\n",
        "    # 3) Plot heatmaps in columns 1..5\n",
        "    for c in range(1, n_cols):\n",
        "        ax = axes[r, c]\n",
        "        idx_letter = r*n_cols + c\n",
        "        ax.text(0.02, 0.90, letters[idx_letter],\n",
        "                transform=ax.transAxes,\n",
        "                fontsize=12, fontweight='bold', color='white')\n",
        "\n",
        "        hm = heatmaps[c-1]\n",
        "        im = ax.imshow(hm, cmap='jet', vmin=0, vmax=1)\n",
        "        ax.set_title(titles[c], fontsize=10)\n",
        "        ax.axis('off')\n",
        "\n",
        "# 4) Shared colorbar for all attribution columns\n",
        "cbar = fig.colorbar(im, ax=axes[:,1:], orientation='vertical',\n",
        "                    fraction=0.02, pad=0.01, label='Attribution intensity')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.94, hspace=0.3)\n",
        "plt.suptitle(\"Stage 2: Originals + Standalone Heatmaps (CAM, IG, LIME)\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "fig.savefig(\"survey_stage2_standalone_heatmaps.png\", dpi=200, bbox_inches=\"tight\")\n",
        "print(\"✅ Saved survey_stage2_standalone_heatmaps.png\")\n"
      ],
      "metadata": {
        "id": "Fb7NvnVro-xM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Cell: 10×6 Survey Examples with Overlays + Standalone Heatmaps (using compute_ig) ─────────────\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# 1) Pick five VOC‐validation indices:\n",
        "idx_list = [10, 88, 122, 450, 1200]\n",
        "\n",
        "# 2) Early vs. late layers:\n",
        "early_layer = model.layer1[0]\n",
        "late_layer  = model.layer4[-1]\n",
        "\n",
        "# 3) Build a figure: 10 rows (5 examples × 2) × 6 columns\n",
        "n_examples = len(idx_list)\n",
        "n_rows     = n_examples * 2\n",
        "n_cols     = 6\n",
        "fig, axes = plt.subplots(n_rows, n_cols,\n",
        "                         figsize=(20, 2 * n_rows),\n",
        "                         dpi=100)\n",
        "\n",
        "# 4) Loop through each example\n",
        "for i, img_idx in enumerate(idx_list):\n",
        "    top = 2 * i      # overlay row\n",
        "    bot = top + 1    # heatmap row\n",
        "\n",
        "    # 4a) Load image & GT\n",
        "    img_pil, gt_boxes = dataset[img_idx]\n",
        "    img_rgb = np.array(img_pil.convert(\"RGB\"))\n",
        "    img_bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n",
        "    H, W    = img_bgr.shape[:2]\n",
        "\n",
        "    # 4b) Predict class\n",
        "    inp_tensor = preprocess(img_pil).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits  = model(inp_tensor)\n",
        "        cls_idx = int(logits.argmax(dim=1).item())\n",
        "        cls_name= weights.meta[\"categories\"][cls_idx]\n",
        "\n",
        "    # 4c) Draw ground-truth boxes\n",
        "    viz = img_rgb.copy()\n",
        "    for x1, y1, x2, y2 in gt_boxes:\n",
        "        cv2.rectangle(viz, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "\n",
        "    # 4d) Compute Grad-CAM at early & late\n",
        "    cam_e, _ = compute_cam(img_bgr, model, preprocess,\n",
        "                           target_layer=early_layer, device=device)\n",
        "    cam_l, _ = compute_cam(img_bgr, model, preprocess,\n",
        "                           target_layer=late_layer, device=device)\n",
        "\n",
        "    # 4e) Compute Saliency\n",
        "    sal_map  = compute_saliency(img_bgr, model, preprocess, device)\n",
        "\n",
        "    # 4f) Compute IG at early & late via compute_ig()\n",
        "    ig_e = compute_ig(img_bgr, model, preprocess, device,\n",
        "                      baseline=None, n_steps=20,\n",
        "                      target_layer=early_layer)\n",
        "    ig_l = compute_ig(img_bgr, model, preprocess, device,\n",
        "                      baseline=None, n_steps=20,\n",
        "                      target_layer=late_layer)\n",
        "\n",
        "    # 4g) Prepare overlays and titles\n",
        "    overlays = [viz, cam_e, cam_l, sal_map, ig_e, ig_l]\n",
        "    titles   = [\n",
        "        f\"Idx {img_idx}\\nPred: {cls_name}\",\n",
        "        \"CAM Early\", \"CAM Late\",\n",
        "        \"Saliency\", \"IG Early\", \"IG Late\"\n",
        "    ]\n",
        "\n",
        "    # --- Overlay row (top) ---\n",
        "    for c in range(n_cols):\n",
        "        ax = axes[top, c]\n",
        "        if c == 0:\n",
        "            ax.imshow(overlays[c])\n",
        "        else:\n",
        "            ax.imshow(viz)\n",
        "            ax.imshow(overlays[c], cmap=\"jet\", alpha=0.5)\n",
        "        ax.set_title(titles[c], fontsize=10)\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    # --- Heatmap row (bot) ---\n",
        "    for c in range(n_cols):\n",
        "        ax = axes[bot, c]\n",
        "        if c == 0:\n",
        "            ax.axis(\"off\")\n",
        "        else:\n",
        "            ax.imshow(overlays[c], cmap=\"jet\")\n",
        "            ax.set_title(\"Heatmap\", fontsize=8)\n",
        "            ax.axis(\"off\")\n",
        "\n",
        "# 5) Layout adjustments and save\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.95, hspace=0.2)\n",
        "plt.suptitle(\"Survey: Overlays (odd rows) + Heatmaps (even rows)\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "fig.savefig(\"survey_examples_with_heatmaps.png\", dpi=200, bbox_inches=\"tight\")\n",
        "print(\"✅ Saved as survey_examples_with_heatmaps.png\")\n"
      ],
      "metadata": {
        "id": "5sV_058TTPyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e5ac2d3"
      },
      "source": [
        "### Cell: Visualize CAM Results\n",
        "\n",
        "Visualize the CAM heatmaps and their overlays on the original image for both the early and late layers of the ResNet model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HFxiM-RA8B07"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dc8e7ac"
      },
      "source": [
        "### Cell: Visualize CAM Results\n",
        "\n",
        "Visualize the CAM heatmaps and their overlays on the original image for both the early and late layers of the ResNet model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}